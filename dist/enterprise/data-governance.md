# Data Governance

> Use when implementing data governance frameworks, building data catalogs, establishing data lineage, defining data quality rules, or setting up data stewardship programs - covers metadata management, data quality, and compliance

**Category:** enterprise | **Version:** 1.0.0

---

## Patterns


## Anti-Patterns


## Sharp Edges (Gotchas)

*Real production issues that cause outages and bugs.*

### [CRITICAL] Critical data has no accountable business owner

**Why it happens:**
Without clear ownership, data quality degrades. When issues arise,
nobody feels responsible for fixing them. Different teams create
conflicting definitions leading to inconsistent reporting.


**Solution:**
```
1. Assign data owners (business leaders):
   - One owner per data domain
   - Authority to make decisions
   - Accountable for quality

2. Assign data stewards (operational):
   - Day-to-day quality monitoring
   - Maintain metadata
   - Escalate issues to owner

3. Document in data catalog:
   - Owner contact information
   - Steward contact information
   - Escalation path

```

**Symptoms:**
- Data quality issues persist unfixed
- Nobody approves access requests
- Conflicting definitions across teams

---

### [HIGH] Catalog doesn't reflect reality so nobody trusts it

**Why it happens:**
A stale data catalog is worse than no catalog. Users waste time
checking the catalog only to find wrong information. Eventually
they stop using it, making the investment worthless.


**Solution:**
```
1. Automated metadata ingestion:
   - Crawl databases on schedule
   - Detect schema changes
   - Alert on drift

2. CI/CD integration:
   - Update catalog on deploy
   - Block deploys without metadata
   - Version descriptions with code

3. Active metadata:
   - Show actual freshness
   - Real-time lineage from logs
   - Usage metrics from queries

```

**Symptoms:**
- Tables exist in DB but not catalog
- Column descriptions outdated
- Lineage shows old pipelines

---

### [CRITICAL] Personal data copied across systems without tracking

**Why it happens:**
GDPR and other regulations require knowing where PII lives.
When data is copied freely, you can't respond to deletion requests,
can't detect breaches accurately, and face compliance risk.


**Solution:**
```
1. PII discovery and classification:
   - Automated scanning for patterns
   - ML-based PII detection
   - Tag PII columns in catalog

2. Data masking:
   - Mask PII for non-prod environments
   - Tokenization for analytics
   - Access controls on sensitive columns

3. Lineage for PII:
   - Track PII data flows
   - Alert on new PII destinations
   - Enable deletion propagation

```

**Symptoms:**
- Customer emails in analytics warehouse
- Export files with unmasked data
- Test environments with production PII

---

### [HIGH] Bad data discovered when reports break

**Why it happens:**
Checking quality at consumption means bad data has already
propagated through the system. Fix is expensive - must trace
back through pipelines and reprocess. Users lose trust.


**Solution:**
```
1. Shift left - check at source:
   - Validate ingestion data
   - Fail pipelines on quality issues
   - Don't propagate bad data

2. Check at each stage:
   - Quality gates between transforms
   - Anomaly detection on row counts
   - Statistical process control

3. Contract testing:
   - Define expectations per dataset
   - Automated quality tests in CI
   - Quarantine failing data

```

**Symptoms:**
- Dashboard shows impossible values
- ML model performance degrades
- Data issues found by business users

---

## Collaboration

### When to Hand Off

| Trigger | Delegate To | Context |
|---------|-------------|--------|
| `enterprise.architecture|data.architecture` | enterprise-architecture | Enterprise data strategy |
| `gdpr|privacy|pii|consent` | gdpr-privacy | Privacy compliance |
| `sox|audit|controls` | sox-compliance | Audit requirements |

### Receives Work From

- **enterprise-architecture**: Enterprise data architecture
- **gdpr-privacy**: Privacy requirements

---

## Get the Full Version

This skill has **automated validations**, **detection patterns**, and **structured handoff triggers** that work with the Spawner orchestrator.

```bash
npx vibeship-spawner-skills install
```

Full skill path: `~/.spawner/skills/enterprise/data-governance/`

**Includes:**
- `skill.yaml` - Structured skill definition
- `sharp-edges.yaml` - Machine-parseable gotchas with detection patterns
- `validations.yaml` - Automated code checks
- `collaboration.yaml` - Handoff triggers for skill orchestration

---

*Generated by [VibeShip Spawner](https://github.com/vibeforge1111/vibeship-spawner-skills)*
