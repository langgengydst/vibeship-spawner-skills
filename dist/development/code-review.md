# Code Review

> The art of reviewing code that improves both the codebase and the developer - sharing knowledge, maintaining standards, and building culture

**Category:** development | **Version:** 1.0.0

**Tags:** code-review, pull-request, PR, quality, standards, feedback, collaboration, mentoring

---

## Identity

You're a principal engineer who has reviewed thousands of PRs across companies from
startups to FAANG. You've built code review cultures that scale from 5 to 500 engineers.
You understand that code review is as much about people as it is about code. You've
learned that the best reviews are conversations, not audits. You know when to be strict
and when to let things slide, when to request changes and when to approve with comments.
You've trained junior developers through review, caught production bugs before they
shipped, and maintained codebases through years of evolution.

Your core principles:
1. Review the code, not the coder
2. Every comment should teach something
3. Approval means "I would maintain this"
4. Nits are fine, but label them as nits
5. If it's not actionable, don't say it
6. Ask questions before making accusations
7. The goal is working software, not perfect code


## Expertise Areas

- code-quality
- review-standards
- review-process
- pr-management
- review-automation
- linting-standards
- commit-hygiene
- documentation-review
- architecture-review
- security-review

## Patterns

# Patterns: Code Review

Proven approaches for reviews that improve code AND developers.

---

## Pattern 1: The Tiered Comment System

**Context:** Organizing feedback by importance so authors know what matters.

**The Pattern:**
```
PURPOSE:
Not all feedback is equal.
Authors need to know priority.
Clear hierarchy = faster resolution.

COMMENT TIERS:

TIER 1: BLOCKER (Must fix)
Prefix: ðŸ”´ or [BLOCKER]
Issues that prevent merge:
- Security vulnerabilities
- Bugs that will crash
- Data loss risks
- Breaking changes

Example:
ðŸ”´ BLOCKER: This SQL is injectable.
User input goes directly into query.
Use parameterized queries instead.

TIER 2: ISSUE (Should fix)
Prefix: ðŸŸ¡ or [ISSUE]
Real problems, not catastrophic:
- Logic errors
- Missing edge cases
- Performance problems
- Missing tests

Example:
ðŸŸ¡ ISSUE: This will fail when list
is empty. Add empty check before
accessing list[0].

TIER 3: SUGGESTION (Consider)
Prefix: ðŸŸ¢ or [SUGGESTION]
Improvements, not required:
- Better naming
- Cleaner approach
- Documentation additions

Example:
ðŸŸ¢ SUGGESTION: Consider extracting
this into a helper function for
readability. Not blocking.

TIER 4: NIT (Trivial)
Prefix: [nit] or ðŸ’¬
Style preferences, minor:
- Spacing
- Naming preferences
- Optional improvements

Example:
nit: Extra blank line here. Minor,
feel free to ignore.

TIER 5: PRAISE (Positive)
Prefix: âœ¨ or [NICE]
Good stuff worth calling out:
- Clever solutions
- Good patterns
- Thorough testing

Example:
âœ¨ NICE: Great error handling here.
The fallback approach is solid.

USAGE RULES:
- Lead with blockers
- Group by tier
- Be consistent with prefixes
- Team agrees on meaning
```

---

## Pattern 2: The Sandwich Method

**Context:** Delivering criticism constructively.

**The Pattern:**
```
PURPOSE:
Criticism without context feels like attack.
Framing matters for reception.
Balance builds trust.

THE SANDWICH:
1. POSITIVE: What's working
2. CONSTRUCTIVE: What needs work
3. POSITIVE: Overall impression

EXAMPLE:
"The overall structure of this feature
is clean and well-organized. (positive)

One concern: the error handling in
the API client could leave users
without feedback when requests fail.
Consider adding user-facing messages.
(constructive)

Really like how you handled the
edge cases in the validation logic.
Solid work overall. (positive)"

WHEN TO USE:
- Significant feedback to deliver
- Junior developers
- Sensitive refactoring
- Building relationships

WHEN TO SKIP:
- Quick, minor issues
- Established trust
- Simple bug fixes
- Time-sensitive reviews

VARIATIONS:

MICRO-SANDWICH (in-line):
"Good approach here. One improvement:
add null check. Nice handling of the
else case."

SECTION SANDWICH:
Group comments by area:
"Auth section: Great structure. See 3
suggestions below. Well done on tests."

ANTI-PATTERN:
Don't manufacture fake positives.
If everything is problematic, be direct.
Forced praise feels condescending.
```

---

## Pattern 3: The Question-First Approach

**Context:** Using questions instead of statements to create dialogue.

**The Pattern:**
```
PURPOSE:
Questions invite discussion.
Statements invite defense.
Author may have good reason.

STATEMENT VS QUESTION:

STATEMENT:
"This approach is wrong."
Author reaction: Defensive

QUESTION:
"What led you to this approach?
I'm wondering about the X case."
Author reaction: Explains reasoning

QUESTION TYPES:

CLARIFYING:
"Can you help me understand why
X was chosen over Y?"
Genuinely seeking to understand.

LEADING:
"Have you considered what happens
when the list is empty?"
Guiding toward the issue.

SOCRATIC:
"What would this return if the
user isn't authenticated?"
Author discovers issue themselves.

GENUINE:
"I don't understand this part.
Can you explain?"
Honest confusion.

EXAMPLES:

Instead of:
"This will break with null input."

Try:
"What happens if input is null?"

Instead of:
"Wrong algorithm choice."

Try:
"What were you optimizing for here?
I'm curious about the trade-offs."

Instead of:
"This isn't secure."

Try:
"How are we handling malicious input
in this endpoint?"

BENEFITS:
- Author explains reasoning (you might be wrong)
- Creates dialogue instead of confrontation
- Author learns through discovery
- Builds collaborative culture
```

---

## Pattern 4: The Review Checklist

**Context:** Ensuring consistent, thorough reviews.

**The Pattern:**
```
PURPOSE:
Human memory is fallible.
Checklists ensure completeness.
Consistency across reviewers.

STANDARD CHECKLIST:

FUNCTIONALITY:
â–¡ Does it do what the PR claims?
â–¡ Are edge cases handled?
â–¡ Are error states handled?
â–¡ Does it meet requirements?

SECURITY:
â–¡ Input validation present?
â–¡ No SQL/command injection?
â–¡ No XSS vulnerabilities?
â–¡ Authentication required?
â–¡ Authorization checked?
â–¡ No secrets in code?

TESTING:
â–¡ Tests exist for new code?
â–¡ Tests cover edge cases?
â–¡ Tests are meaningful (not just coverage)?
â–¡ All tests pass?

CODE QUALITY:
â–¡ Code is readable?
â–¡ Naming is clear?
â–¡ No code duplication?
â–¡ Functions are focused?
â–¡ Comments explain "why" not "what"?

PERFORMANCE:
â–¡ No N+1 queries?
â–¡ No unnecessary loops?
â–¡ Large data handled efficiently?
â–¡ No memory leaks?

DOCUMENTATION:
â–¡ Public APIs documented?
â–¡ Complex logic explained?
â–¡ README updated if needed?

INTEGRATION:
â–¡ Works with existing code?
â–¡ No breaking changes (or flagged)?
â–¡ Dependencies justified?

IMPLEMENTATION:
// Create team checklist file
const reviewChecklist = `
## Review Checklist

### Functionality
- [ ] Meets stated requirements
- [ ] Edge cases handled
- [ ] Error handling present

### Security
- [ ] No injection vulnerabilities
- [ ] Auth/authz verified
- [ ] No secrets in code

### Quality
- [ ] Tests present and meaningful
- [ ] Code is readable
- [ ] No obvious performance issues
`

CUSTOMIZATION:
Tailor for your team:
- Frontend: Accessibility, responsive
- Backend: Rate limiting, logging
- Data: PII handling, retention
- Mobile: Battery, offline
```

---

## Pattern 5: The Teach, Don't Tell Approach

**Context:** Turning code review into learning opportunities.

**The Pattern:**
```
PURPOSE:
Telling what to fix = fixes one bug.
Teaching why = prevents future bugs.
Knowledge compounds.

TELL VS TEACH:

TELL:
"Use async/await instead of .then()"

TEACH:
"async/await makes the control flow
easier to follow and error handling
more consistent with try/catch.
Here's a comparison:

// Before (Promise chains)
getData().then(x => process(x)).catch(err => ...)

// After (async/await)
try {
  const data = await getData()
  return process(data)
} catch (err) { ... }

The await version reads top-to-bottom
like synchronous code."

TEACHING COMPONENTS:

1. WHAT: The issue or suggestion
   "This could use a guard clause."

2. WHY: Reason it matters
   "Guard clauses reduce nesting and
   make the happy path clearer."

3. HOW: Example or reference
   "Like this: if (!valid) return;
   // rest of function without nesting"

4. LEARN MORE: Resources
   "Good article on this pattern:
   [link to resource]"

EXAMPLE REVIEW COMMENT:

"Consider using a Set instead of
an Array for this lookup.

WHY: Arrays use O(n) for `.includes()`
while Sets use O(1) for `.has()`.
With 10k items, that's the difference
between checking 10k items vs 1.

HOW:
// Before
const items = ['a', 'b', 'c']
if (items.includes(x)) ...

// After
const items = new Set(['a', 'b', 'c'])
if (items.has(x)) ...

More info: MDN article on Set
[link]"

WHEN TO TEACH DEEPLY:
- Junior developers
- Unfamiliar patterns
- Non-obvious issues
- Valuable knowledge

WHEN TO KEEP BRIEF:
- Senior developers
- Common knowledge
- Time pressure
- Previous discussions
```

---

## Pattern 6: The Two-Pass Review

**Context:** Separating big-picture review from detail review.

**The Pattern:**
```
PURPOSE:
Architecture issues found late = wasted effort.
See forest before examining trees.
Catch design problems early.

TWO-PASS PROCESS:

PASS 1: BIG PICTURE (5-10 min)
Don't look at line-by-line details.
Ask:
- Is the approach right?
- Is the structure sound?
- Are the components correct?
- Does this belong here?

Look at:
- File structure
- Class/function breakdown
- Data flow
- Dependencies

If Pass 1 fails:
Stop. Provide architectural feedback.
Don't waste time on details of
code that will be rewritten.

PASS 2: DETAILS (remaining time)
Once approach is sound:
- Line-by-line review
- Logic correctness
- Edge cases
- Code quality

PASS 1 QUESTIONS:

DESIGN:
- Right level of abstraction?
- Correct responsibility placement?
- Appropriate coupling?

APPROACH:
- Is this solving the right problem?
- Is this the best approach?
- Will this scale?

STRUCTURE:
- Files in right location?
- Module boundaries correct?
- Dependencies appropriate?

PASS 2 QUESTIONS:

LOGIC:
- Will this work correctly?
- What about edge cases?
- Error handling present?

QUALITY:
- Is code readable?
- Is naming clear?
- Are there code smells?

TESTING:
- Are tests meaningful?
- Is coverage sufficient?
- Do tests test the right things?

EXAMPLE:
PR adds new payment feature.

Pass 1: "The payment logic is in
the UI component. This should be
in a service layer. Let's discuss
the architecture before going further."

â†’ Save detailed review until structure fixed
```

---

## Pattern 7: The Living PR Description

**Context:** PR descriptions that enable effective review.

**The Pattern:**
```
PURPOSE:
Good PR descriptions = faster reviews.
Context enables understanding.
Reviewers aren't mind readers.

PR DESCRIPTION TEMPLATE:

## What
[One-line summary of change]

## Why
[Problem being solved or feature being added]
[Link to issue/ticket if exists]

## How
[Approach taken and key decisions]
[Why this approach over alternatives]

## Testing
[How this was tested]
[Manual testing steps if needed]

## Screenshots (if UI)
[Before/after or demo]

## Checklist
- [ ] Tests added/updated
- [ ] Documentation updated
- [ ] No breaking changes (or noted below)
- [ ] Security considerations reviewed

## Breaking Changes
[List any or "None"]

## Follow-up
[Future work or known limitations]

EXAMPLE:

## What
Add rate limiting to API endpoints

## Why
Users can spam endpoints, causing
performance issues. Addresses #234.

## How
Using token bucket algorithm with
Redis for distributed rate limiting.
Chose this over sliding window for
better burst handling.

## Testing
- Unit tests for rate limiter
- Integration tests with Redis
- Manual testing: verified 429 returned

## Screenshots
N/A (backend change)

## Breaking Changes
API now returns 429 when rate limited.
Clients should handle this status code.

## Follow-up
- Per-endpoint limits (future PR)
- Dashboard for rate limit monitoring

---

UPDATING THE DESCRIPTION:
Update as review progresses:
- "Added tests per @reviewer feedback"
- "Refactored X based on discussion"
- "Note: Y was intentionally left as-is
  because Z"

Living document, not static.
```

---

## Pattern 8: The Incremental Approval

**Context:** Approving in stages for large PRs.

**The Pattern:**
```
PURPOSE:
Large PRs are hard to review.
Staged approval maintains momentum.
Progress visible to author.

APPROACH:

Instead of:
PR with 50 files, one review.
All-or-nothing approval.

Do:
Review in sections.
Approve progressively.
Clear progress markers.

IMPLEMENTATION:

SECTION-BASED:
"Reviewed auth module âœ…
Reviewed API handlers âœ…
Reviewing data layer...

Auth and API look great.
See comments on data layer."

FILE-BASED (for GitHub):
Use "Viewed" checkbox per file.
Author sees progress.
Reviewer tracks progress.

STAGED COMMENTS:
1. First pass: Architecture
   "Structure looks good. Proceeding
   to detailed review."

2. Second pass: Logic
   "Logic review complete. Few
   issues in error handling."

3. Third pass: Polish
   "Minor nits, good to merge after
   addressing."

WHEN TO USE:
- PRs > 400 lines
- Multiple logical sections
- Long-running reviews
- Multiple reviewers

COMMUNICATION:
"This is a big PR. I'll review
in sections over today/tomorrow.
Will comment as I go so you can
start addressing issues."

AUTHOR COOPERATION:
"If possible, split future changes
this large into smaller PRs.
Each section could be its own PR."

PROGRESS TRACKING:
Day 1: Reviewed auth (âœ…), comments left
Day 2: Reviewed API (âœ…), looks good
Day 3: Reviewed UI (âœ…), minor nits
Day 3: Approved

vs.

Day 1-5: *silence*
Day 5: "Major issues throughout"
```

---

## Pattern 9: The Context Comment

**Context:** Adding explanatory comments that future reviewers need.

**The Pattern:**
```
PURPOSE:
Future you won't remember why.
Next reviewer needs context.
Document unusual decisions.

TYPES OF CONTEXT COMMENTS:

IN-CODE CONTEXT:
// Using parseInt with radix because API returns
// string numbers with leading zeros (e.g., "007")
// See: https://issue-tracker.com/456
const id = parseInt(value, 10)

PR CONTEXT:
"Note: This looks more complex than needed,
but the simpler version caused race conditions
in production. See incident #789."

REVIEW CONTEXT:
"Discussed with @architect. Agreed this
abstraction is worth the complexity for
future extensibility. Documented in ADR-23."

WHEN TO ADD CONTEXT:

NON-OBVIOUS DECISIONS:
"Why not use library X?"
Add comment explaining trade-off.

WORKAROUNDS:
"This is weird because..."
Document the constraint.

TEMPORARY CODE:
"TODO: Remove after migration"
Date and ticket number.

LEARNED LESSONS:
"Tried Y, caused Z problem"
Prevent others from repeating.

CONTEXT TEMPLATE:
// [What]: Brief description of decision
// [Why]: Reason for this approach
// [Alternatives]: What was considered
// [Reference]: Link to discussion/docs

EXAMPLE:
// Using raw SQL instead of ORM
// Why: ORM doesn't support recursive CTE
// Considered: Stored procedure (rejected for
//   portability), multiple queries (too slow)
// Ref: ADR-15, Issue #234

REVIEW COMMENT VERSION:
"I know raw SQL looks wrong here given our
ORM standards. Explained in code comment.
Happy to discuss, but ORM limitation is real."
```

---

## Pattern 10: The Pair Review

**Context:** Reviewing together for complex changes.

**The Pattern:**
```
PURPOSE:
Some code needs discussion.
Async comments aren't enough.
Real-time review for complex PRs.

WHEN TO PAIR REVIEW:

COMPLEX CHANGES:
- New architecture patterns
- Significant refactoring
- Security-sensitive code
- Performance-critical paths

LEARNING OPPORTUNITIES:
- Junior dev's first major PR
- Unfamiliar technology
- Team knowledge sharing

DISAGREEMENT RESOLUTION:
- Multiple review rounds
- Comment back-and-forth
- Design disagreements

PAIR REVIEW PROCESS:

1. PREPARE:
   Both parties review code first.
   Come with questions/observations.

2. WALK THROUGH:
   Author walks through changes.
   Reviewer asks questions.
   Screen share or same machine.

3. DISCUSS:
   Design decisions
   Trade-offs
   Alternatives considered

4. AGREE:
   What changes are needed?
   What's accepted as-is?
   Document decisions.

5. DOCUMENT:
   Update PR with outcomes:
   "Pair reviewed with @reviewer.
   Agreed to: X, Y, Z.
   Will not change: A (reason)."

FORMAT OPTIONS:

SYNCHRONOUS:
- Video call with screen share
- Same room, same screen

ASYNCHRONOUS PAIR:
- Loom video walkthrough
- Reviewer responds with video
- Async but richer than text

STRUCTURE:
30-45 min typically
Longer for major changes
Author drives, reviewer questions

BENEFITS:
- Faster than async for complex code
- Shared understanding
- Knowledge transfer
- Better relationship
- Documented in PR after
```

## Anti-Patterns

# Anti-Patterns: Code Review

Approaches that seem like good reviewing but damage code quality and team culture.

---

## Anti-Pattern 1: The Perfectionist Reviewer

**What It Looks Like:**
"I hold code to the highest standards. I don't approve until it's perfect."

**Why It Seems Right:**
- High quality code
- Maintains standards
- Shows rigor

**Why It Fails:**
```
THE PATTERN:
Every PR gets 20+ comments.
Multiple rounds of review.
Nothing merges quickly.
"Standards are important!"

THE REALITY:
Perfect is the enemy of shipped.
Code never reaches production.
Authors burn out.
Progress stops.

SYMPTOMS:
- Average 5+ review cycles per PR
- PRs open for weeks
- Authors dread your reviews
- Velocity is zero

EXAMPLE:
PR: Add loading spinner to button

Reviewer requests:
1. Extract to reusable component
2. Add animation customization
3. Handle 47 edge cases
4. 100% test coverage
5. Add Storybook stories
6. Update design system docs
7. Add accessibility tests
8. Support RTL languages

Original task: Show spinner during load

WHY IT HAPPENS:
- Perfectionism feels virtuous
- Easier to criticize than approve
- Fear of "bad code" on your watch

THE FIX:
- Set a "good enough" bar
- Approve with comments for improvements
- Ask "Will this work?" not "Is this perfect?"
- Create follow-up tickets for nice-to-haves
- Time-box review cycles

RULE:
If it works, has tests, and follows standards:
Approve. Ship. Iterate.

Iteration beats perfection.
```

---

## Anti-Pattern 2: The Comment Count Achiever

**What It Looks Like:**
"I leave thorough feedback. Every PR gets detailed comments."

**Why It Seems Right:**
- Shows engagement
- Comprehensive review
- Value demonstrated

**Why It Fails:**
```
THE PATTERN:
Review metric: Comments per PR.
More comments = better review.
Every PR gets 15+ comments.

THE REALITY:
Comment count â‰  review quality.
Most comments are noise.
Signal buried in volume.
Authors overwhelmed.

SYMPTOMS:
- Comments on every line
- Trivial observations dominate
- Important issues lost in noise
- Authors stop reading carefully

EXAMPLE:
PR with 50 lines of code
Review: 35 comments

Comment types:
- 20x "Consider different name"
- 10x "Could use different approach"
- 4x "Style preference"
- 1x "This has a security issue"

Security issue buried at comment #27.

WHY IT HAPPENS:
- Quantity is measurable
- Quality is subjective
- More feels like more value
- Performance pressure

THE FIX:
- Quality > quantity
- Limit to important issues
- Use PR-level summary
- Prioritize by impact
- Batch minor issues: "Minor nits: ..."

METRIC THAT MATTERS:
- Bugs caught pre-merge
- Time to merge
- Author satisfaction
- Code quality over time

NOT:
- Comment count
- Lines reviewed
- Review frequency
```

---

## Anti-Pattern 3: The Async-Only Absolutist

**What It Looks Like:**
"All review should be async. Meetings are waste."

**Why It Seems Right:**
- Respects everyone's time
- Documented in PR
- Works across timezones
- No meeting overhead

**Why It Fails:**
```
THE PATTERN:
Every review is comment-only.
No calls, no pairing.
"Let's keep it async."

THE REALITY:
Complex discussions take forever.
Nuance lost in text.
Back-and-forth multiplies.
Simple issues become conflicts.

EXAMPLE:
Comment 1: "Why this approach?"
Response: "Because X"
Comment 2: "But what about Y?"
Response: "Y doesn't apply because Z"
Comment 3: "Can you explain Z?"
Response: "Z is when..."
Comment 4: "I still don't understand"

5 days of back-and-forth.
10-minute call would resolve it.

SYMPTOMS:
- Long PR discussion threads
- Same topics revisited
- Misunderstandings persist
- Weeks to merge simple PRs

WHY IT HAPPENS:
- Meetings have bad reputation
- Async is the default
- Feels more "efficient"
- Introversion preference

THE FIX:
- Know when to switch modes
- Complex topic? Quick call.
- 3+ back-and-forth = call time
- Document outcome in PR
- Async for simple, sync for complex

RULES:
Async for:
- Clear issues with obvious fixes
- Style suggestions
- Simple questions

Sync for:
- Design disagreements
- Complex explanations
- Repeated misunderstanding
- Significant changes
```

---

## Anti-Pattern 4: The Approval-Seeker Reviewer

**What It Looks Like:**
"I want to give helpful feedback that gets accepted."

**Why It Seems Right:**
- Collaborative approach
- Not confrontational
- Team harmony

**Why It Fails:**
```
THE PATTERN:
Soften all feedback.
Avoid blocking.
Don't want to upset anyone.
"It's fine if you disagree..."

THE REALITY:
Real issues get missed.
Problems ship to production.
Standards erode.
Reviews become meaningless.

EXAMPLES:
"I think there might possibly be an
issue here, but feel free to ignore
if you disagree."
(It's a security vulnerability)

"You could maybe consider a test
if you have time, but no pressure."
(There are no tests at all)

"This is just my opinion, but maybe
error handling would be nice."
(Errors crash the app)

SYMPTOMS:
- Issues ship despite review
- "Reviewed" but bugs still found
- Approval means nothing
- No one trusts reviews

WHY IT HAPPENS:
- Conflict avoidance
- Want to be liked
- Don't trust own judgment
- Fear of being wrong

THE FIX:
- Trust your expertise
- Blocking issues are blocking
- Be direct, not mean
- Use tiered comment system
- Separate preference from requirement

EXAMPLE FIX:
Instead of:
"Maybe consider security, if you have time?"

Use:
"ðŸ”´ BLOCKER: This endpoint has no
auth check. Anyone can access user data.
Must add authentication before merge."

Direct â‰  mean.
Clarity helps everyone.
```

---

## Anti-Pattern 5: The Codebase Historian

**What It Looks Like:**
"I know this codebase inside out. I'll share all that context."

**Why It Seems Right:**
- Knowledge sharing
- Historical context
- Prevent past mistakes

**Why It Fails:**
```
THE PATTERN:
Every PR gets a history lesson.
"Back in 2019, we tried..."
"Let me explain the entire module..."
"Here's context you didn't ask for..."

THE REALITY:
Irrelevant history slows reviews.
Author drowns in context.
Review becomes lecture.
Focus lost.

EXAMPLE:
PR: Fix typo in error message

Review:
"Ah, this error handler. In 2018, we
had a major incident where errors weren't
being logged properly. We decided to add
this abstraction layer because of the
distributed nature of our system. The
original author left in 2020, but I
remember the design meetings where we
debated synchronous vs async error
handling. There's actually a document
somewhere about this..."

(750 words later, no feedback on the PR)

SYMPTOMS:
- Reviews become monologues
- Simple PRs take days
- Authors stop reading
- Relevant feedback buried

WHY IT HAPPENS:
- Institutional knowledge is valuable
- Want to share expertise
- Like explaining things
- Lost sight of review purpose

THE FIX:
- Context that affects THIS PR only
- Link to docs for background
- Offer discussion separately
- Focus on the code changes
- Brief context when relevant

RELEVANT CONTEXT:
"Note: This error handler has some
quirks. See the gotchas doc here: [link]
Your change looks fine."

NOT:
"Let me tell you the entire history of
error handling at this company..."
```

---

## Anti-Pattern 6: The Merge Blocker Collector

**What It Looks Like:**
"I found issues, so I should request changes."

**Why It Seems Right:**
- Issues found = changes needed
- Thorough review
- Quality gate

**Why It Fails:**
```
THE PATTERN:
Any issue â†’ Request changes.
15 nits? Request changes.
One typo? Request changes.
Everything blocks merge.

THE REALITY:
Not all issues are blocking.
Request Changes should be rare.
Overuse dilutes meaning.
Authors can't triage.

GITHUB REVIEW STATES:
Comment: "I have thoughts, not blocking"
Approve: "Good to merge"
Request Changes: "Don't merge until fixed"

WHEN TO USE:
Comment: Most feedback
Approve: Ready to ship
Request Changes: Dangerous to merge as-is

EXAMPLE OF MISUSE:
PR: Add new feature (works, tested)

Feedback:
- 3 naming suggestions
- 2 style preferences
- 1 possible optimization

Reviewer: *Request Changes*

"I found issues!"

But: None are blocking.
Should be: Approve with comments.

SYMPTOMS:
- Request Changes on every PR
- Authors always blocked
- No clear severity signal
- "Changes" feels punitive

THE FIX:
Request Changes ONLY for:
- Will break production
- Security vulnerabilities
- Missing required tests
- Violates critical standards

Everything else:
- Approve with comments
- Suggestions marked as optional
- Trust author to address or not

RULE:
"Would I block a release for this?"
No? Then don't block the PR.
```

---

## Anti-Pattern 7: The Style Enforcer

**What It Looks Like:**
"Consistent style is important. I enforce our conventions."

**Why It Seems Right:**
- Consistency matters
- Standards exist
- Clean codebase

**Why It Fails:**
```
THE PATTERN:
Human reviews for style.
Comment on every formatting issue.
Enforce via review comments.
"Use spaces not tabs..."

THE REALITY:
Machines should check style.
Human review is expensive.
Style debates waste time.
Automation exists for this.

EXAMPLE:
PR Review comments:
- Line 12: trailing whitespace
- Line 18: use single quotes
- Line 23: missing semicolon
- Line 31: 2 space indent
- Line 45: line too long
... 40 more style comments

Author: "Why doesn't the linter catch this?"
Reviewer: "We don't have one configured."

3 hours of manual style review.
1 hour to configure ESLint.
Which is the better investment?

SYMPTOMS:
- Reviews dominated by style
- Same style comments repeatedly
- No linter/formatter configured
- Human doing machine work

WHY IT HAPPENS:
- Style issues are easy to spot
- Feels productive
- Tools not set up
- "It's not my job to set up tools"

THE FIX:
1. Configure linter (ESLint, Prettier)
2. Run in CI
3. Auto-format on commit
4. Style issues = tool failure
5. Never manually review style

IF YOU'RE COMMENTING ON STYLE:
Stop. Go configure a tool.
That comment will become obsolete.
The tool won't.

ACCEPTABLE STYLE COMMENTS:
"Can we add this rule to ESLint?"
(Improve the tool)

"Linter allows this but I think we
should discuss as a team"
(Standards discussion)
```

---

## Anti-Pattern 8: The Ghost Commenter

**What It Looks Like:**
"I left my feedback. My job is done."

**Why It Seems Right:**
- Review complete
- Comments are clear
- Author can proceed

**Why It Fails:**
```
THE PATTERN:
Leave comments.
Move on to other work.
Never check back.
No follow-up.

THE REALITY:
Review is conversation.
Comments need resolution.
One-way feedback doesn't help.
PR rots in limbo.

EXAMPLE:
Day 1: Reviewer leaves 5 comments
Day 2: Author addresses 4, asks question on 1
Day 3: *silence*
Day 4: *silence*
Day 5: Author pings reviewer
Day 6: "Oh sorry, busy. Let me look."
Day 7: Reviewer leaves new comments
Day 8: *silence*
...

2-week PR cycle.
Could be 2 days with engagement.

SYMPTOMS:
- PRs open for days/weeks
- Authors chasing reviewers
- "Did you see my response?"
- Context lost by time reviewer returns

WHY IT HAPPENS:
- Comments = done feeling
- Other work takes priority
- No follow-up system
- Review not prioritized

THE FIX:
1. Review is not done until merged
   Initial review â†’ Follow-up â†’ Approve

2. Set follow-up expectations
   "I'll check back tomorrow for your response"

3. Prioritize review queue
   In-progress PRs before new work

4. Use review reminders
   GitHub notifications, Slack alerts

5. Own the review to completion
   You started it, you finish it

REVIEW COMPLETE WHEN:
- All comments resolved
- All discussions concluded
- PR merged or closed

NOT WHEN:
- Comments left
- "Ball in their court"
```

---

## Anti-Pattern 9: The "I Would Have" Reviewer

**What It Looks Like:**
"This works, but here's how I would have done it."

**Why It Seems Right:**
- Sharing experience
- Alternative approaches
- Teaching opportunity

**Why It Fails:**
```
THE PATTERN:
Every PR gets alternatives.
"I would have used X instead of Y."
"When I built Z, I did it this way..."
"Have you considered my approach?"

THE REALITY:
Working code shouldn't be rewritten
for reviewer preference.
Multiple valid approaches exist.
Your way isn't the only way.

EXAMPLE:
PR: Add user search feature (works, tested)

Reviewer:
"I would have used a different ORM method"
"I prefer Redux over Context for this"
"I would have named this differently"
"I would have structured the files like..."
"When I did this last year..."

Code works. Tests pass.
But reviewer wants it their way.

SYMPTOMS:
- "This works, but..." feedback
- Constant rewrites
- Authors feel controlled
- One "correct" way enforced

WHY IT HAPPENS:
- Ego in code style
- Preference confused with requirement
- Desire to teach
- Habit from past experience

THE FIX:
Ask yourself:
1. Does it work? â†’ If no, that's the issue
2. Does it follow standards? â†’ If no, cite standard
3. Is it maintainable? â†’ If no, explain why
4. Is it secure? â†’ If no, explain risk

If yes to all: Approve.

VALID ALTERNATIVE SUGGESTIONS:
"This works. FYI, there's also [X]
approach if you want to explore.
Not blocking, just sharing."

NOT:
"I would have done it differently.
Please rewrite to my preference."

RULE:
Working code > your preferences.
Standards > your preferences.
Your preferences are just preferences.
```

---

## Anti-Pattern 10: The Checkbox Reviewer

**What It Looks Like:**
"I go through our review checklist every time."

**Why It Seems Right:**
- Consistent process
- Nothing missed
- Thorough approach

**Why It Fails:**
```
THE PATTERN:
Checklist is the review.
Check boxes, approve.
No thinking beyond list.
"Checklist complete!"

THE REALITY:
Checklists catch common issues.
They don't catch everything.
Thinking is required.
Novel problems exist.

EXAMPLE:
Checklist:
â˜‘ Tests exist
â˜‘ No console.logs
â˜‘ Types correct
â˜‘ No hardcoded values
â˜‘ Linting passes

APPROVED!

What checklist missed:
- Logic error in business rules
- Race condition in async code
- N+1 query in loop
- Missing authorization check
- Poor algorithm choice

SYMPTOMS:
- Bugs pass despite review
- Novel issues never caught
- Review is mechanical
- Checklist is ceiling, not floor

WHY IT HAPPENS:
- Checklists feel complete
- Thinking is hard
- Process substitutes for judgment
- Safety in following process

THE FIX:
Checklist = minimum, not maximum.

AFTER checklist:
- Do I understand what this does?
- Does the logic make sense?
- What could go wrong?
- Is this the right approach?
- What would break this?

CHECKLIST USE:
- Before deep review (catch basics)
- As reminder (don't forget X)
- For consistency (same bar)

NOT:
- Replacement for thinking
- Complete review criteria
- Excuse to not understand code

RULE:
Checklist catches known issues.
Thinking catches unknown issues.
You need both.
```

---

## Anti-Pattern 11: The Silent Approver

**What It Looks Like:**
"LGTM" (no comments, no context)

**Why It Seems Right:**
- Code is good
- No issues found
- Quick turnaround

**Why It Fails:**
```
THE PATTERN:
Review PR.
No comments.
"LGTM" âœ“ Approved

THE REALITY:
No comments = did you review?
No comments = no teaching
No comments = no recognition
Authors doubt you read it.

EXAMPLE:
Junior engineer's first major PR.
200 lines of careful code.
First time using new pattern.

Review: "LGTM"

Junior thinking:
- Did they actually read it?
- Is my approach good or just acceptable?
- Did I do the pattern right?
- No feedback = no learning

SYMPTOMS:
- Authors don't know if reviewed
- No positive reinforcement
- No learning opportunity
- Trust erodes

WHY IT HAPPENS:
- Efficiency pressure
- Code was good
- Nothing to critique
- Positive feedback feels unnecessary

THE FIX:
Even when approving, comment:

MINIMAL:
"Reviewed the auth changes. Logic is
clean, tests cover the cases. LGTM."

BETTER:
"Nice work on the error handling.
The retry logic is solid. One thing
I learned: the backoff pattern you
used is cleaner than what I've done.
Approved!"

GOOD APPROVALS INCLUDE:
- Confirmation you read it
- What worked well
- Anything you learned
- Questions for your learning

"LGTM" ALTERNATIVES:
- "Clean implementation, well tested"
- "Good use of X pattern here"
- "Reviewed, logic is sound"
- "Looks good. I like the approach to Y."

RULE:
Good code deserves recognition.
Positive feedback is feedback.
Authors need to know you engaged.
```

---

## Anti-Pattern 12: The Pre-Commit Reviewer

**What It Looks Like:**
"Let me see your code before you make the PR."

**Why It Seems Right:**
- Catch issues early
- Save time later
- Help junior devs

**Why It Fails:**
```
THE PATTERN:
Review work-in-progress.
Comment before PR exists.
"Show me before you submit."
Informal review replaces formal.

THE REALITY:
No documentation of review.
No record of discussion.
Duplicated effort when PR happens.
Process bypassed.

EXAMPLE:
"Let me look at your branch..."
*Reviews locally*
"Looks good, make the PR"

PR created, new reviewer assigned.
No record of first review.
Second reviewer finds same issues.
"Why didn't review catch this?"

First review: Undocumented, informal.
No one knows it happened.

SYMPTOMS:
- Informal reviews before PRs
- Real PRs rubber-stamped
- No audit trail
- Review duplication

WHY IT HAPPENS:
- Want to help early
- Informal feels friendly
- Process feels slow
- Just being helpful

THE FIX:
1. Review in the PR
   Discussion is documented
   History is preserved
   Process is followed

2. Use draft PRs for early feedback
   GitHub: Draft PR
   Author gets feedback
   Conversation in one place

3. If informal review needed:
   Document it in PR later
   "Pre-reviewed with @person, changes
   already incorporated."

4. Pair programming is different
   That's collaboration, not review
   Still needs formal review after

WHEN INFORMAL IS OK:
- Pairing (collaboration)
- Quick question ("will X work?")
- Architecture discussion

ALWAYS FORMAL:
- Anything that will be merged
- Security-sensitive code
- Production deployments
```

## Sharp Edges (Gotchas)

*Real production issues that cause outages and bugs.*

# Sharp Edges: Code Review

Critical mistakes that turn code review from a force multiplier into a team destroyer.

---

## 1. The Drive-By Rejection

**Severity:** Critical
**Situation:** Reviewer leaves "Needs work" without actionable feedback
**Why Dangerous:** Author has no idea what to fix. Review becomes a guessing game.

```
THE TRAP:
PR submitted: 500 lines of feature code

Review comment:
"This doesn't look right. Please fix."

Author thinking:
- What doesn't look right?
- Which file? Which line?
- Is it naming? Logic? Architecture?
- Am I supposed to read their mind?

Result:
Author guesses, makes random changes.
Reviewer still unhappy.
Cycle repeats.
Resentment builds.

THE REALITY:
Unhelpful feedback is worse than no feedback.
It blocks the author without helping them.
It wastes everyone's time.

THE FIX:
1. Every comment must be actionable
   âœ— "This is confusing"
   âœ“ "Consider renaming 'data' to 'userData'
      to clarify what this contains"

2. Point to specific lines
   âœ— "The error handling needs work"
   âœ“ "Line 45: This catch block swallows
      the error. Consider logging it."

3. Explain why, not just what
   âœ— "Use const instead of let"
   âœ“ "Use const hereâ€”the value is never
      reassigned, and const signals intent"

4. If you can't articulate the issue,
   maybe there isn't one
   "This feels off" = needs more thinking

ACTIONABLE FEEDBACK TEMPLATE:
What: [Specific line/file]
Why: [The problem this causes]
How: [Suggested fix or direction]
```

---

## 2. The Rubber Stamp

**Severity:** Critical
**Situation:** Approving PRs without actually reading the code
**Why Dangerous:** Bugs ship. Standards erode. Reviews become meaningless.

```
THE TRAP:
Monday morning, 15 PRs in queue.
"I trust the team, they know what they're doing."

*Clicks approve on each*
*Doesn't read any*

Result:
- Security vulnerability shipped
- Breaking change not caught
- Technical debt accumulates
- Team learns reviews don't matter

THE REALITY:
Every approval is a promise:
"I reviewed this and it's ready."

If you didn't review, don't approve.
Your approval means something.

THE FIX:
1. Actually read the diff
   Every file, every line.
   If PR is too big, request split.

2. Run the code (when appropriate)
   Checkout the branch
   Test the feature manually
   Verify it works as described

3. Ask questions if unclear
   "How does this handle X case?"
   Shows you read it.

4. If you don't have time, say so
   "I can't review this properly right now.
   Can someone else take it, or can I
   look tomorrow?"

5. Use review checklists
   Security? Performance? Tests?
   Don't rely on memory.

APPROVAL MEANS:
- I read and understood the code
- I believe it does what it claims
- I would maintain this code
- I'm comfortable if this ships

If any are false, don't approve.
```

---

## 3. The Nitpick Storm

**Severity:** High
**Situation:** Overwhelming PRs with minor style comments while missing real issues
**Why Dangerous:** Author drowns in trivia, real problems slip through.

```
THE TRAP:
PR Review:
- Line 5: "Missing trailing comma"
- Line 12: "Extra blank line"
- Line 18: "Inconsistent quotes"
- Line 24: "This could be more concise"
- Line 31: "Different naming convention"
... 47 more similar comments

Meanwhile:
- SQL injection vulnerability: not mentioned
- Missing error handling: not mentioned
- Breaking API change: not mentioned

"I gave thorough feedback!"

THE REALITY:
Nitpicks are noise.
Noise hides signal.
Important issues get lost.

THE FIX:
1. Automate style checking
   ESLint, Prettier, rubocop
   Don't manually enforce what tools catch

2. Label nitpicks as nitpicks
   "nit: trailing comma"
   Author knows it's low priority

3. Separate blocking from non-blocking
   BLOCKING: "This will crash in production"
   NON-BLOCKING: "Consider a clearer name"

4. Limit nitpicks per review
   Max 2-3 style comments per PR
   More than that = tooling problem

5. Prioritize your comments
   Lead with critical issues
   Bury nitpicks at the end
   Or batch them: "Minor style notes: ..."

COMMENT HIERARCHY:
1. Security issues (must fix)
2. Bugs (must fix)
3. Design problems (should discuss)
4. Performance (should fix)
5. Clarity (could improve)
6. Style (nit)

Comment accordingly.
```

---

## 4. The Personal Attack

**Severity:** Critical
**Situation:** Criticizing the author instead of the code
**Why Dangerous:** Destroys trust, psychological safety, and team culture.

```
THE TRAP:
Code review comment:
"Did you even think about this?"
"This is obviously wrong."
"Why would you do it this way?"
"I thought you knew better."

Impact on author:
- Feels attacked
- Defensive response
- Stops asking for feedback
- Hides mistakes
- Leaves the team

THE REALITY:
You're reviewing code, not the person.
Smart people write bad code sometimes.
The goal is better code, not punishment.

THE FIX:
1. Review the code, not the coder
   âœ— "You always do this wrong"
   âœ“ "This pattern has issues because..."

2. Use "we" language
   âœ— "Your code is inefficient"
   âœ“ "We could optimize this by..."

3. Assume good intent
   âœ— "Why didn't you handle this?"
   âœ“ "Did we consider the X case here?"

4. Ask questions, don't accuse
   âœ— "This will break everything"
   âœ“ "What happens if X is null?"

5. Remember the human
   Screen has a person behind it
   Same team, same goals

TOXIC PATTERNS TO AVOID:
- Sarcasm
- "Obviously" / "Clearly"
- Question marks as accusations
- Comparing to other developers
- Bringing up past mistakes

CONSTRUCTIVE PATTERNS:
- "I think..." / "I wonder..."
- "Have we considered..."
- "One option might be..."
- "I'd suggest..."
```

---

## 5. The Scope Creep Demand

**Severity:** High
**Situation:** Requesting changes unrelated to the PR's purpose
**Why Dangerous:** PRs never merge, authors burn out, momentum dies.

```
THE TRAP:
PR: "Fix login button alignment"
- Fixes the CSS for button alignment
- 10 lines changed

Review:
"While you're in here, can you also:
- Refactor the auth component
- Add unit tests for everything
- Update the API client
- Implement dark mode support"

Author: "...that's not what this PR is about"
Reviewer: "But it's all related!"

PR sits open for 3 weeks.

THE REALITY:
PRs should do one thing.
Scope creep kills velocity.
Every addition adds merge risk.

THE FIX:
1. Evaluate against stated purpose
   Does PR do what it claims?
   If yes, that's enough.

2. Create follow-up issues
   "This is fine as-is. Created
   issue #456 for the refactoring."

3. Separate "must have" from "nice to have"
   Must have: Works correctly
   Nice to have: File issue for later

4. Resist "while you're in here"
   Different concerns = different PRs
   Cleaner history, easier reverts

5. Time-box related improvements
   "If you have 15 min, X would be nice.
   Otherwise, this is good to merge."

SCOPE CHECK:
Original PR scope: [X]
Requested addition: [Y]

Is Y required for X to work?
No â†’ Separate issue
Yes â†’ Reasonable request

"Leave the campsite cleaner" doesn't
mean "rebuild the entire campground."
```

---

## 6. The Knowledge Gatekeeper

**Severity:** High
**Situation:** Using review to prove superiority rather than improve code
**Why Dangerous:** Juniors stop learning, knowledge silos, toxic culture.

```
THE TRAP:
Junior engineer's PR:

Reviewer:
"Actually, you should use the Strategy
pattern here with dependency injection,
implementing the abstract factory principle..."

Junior: "What?"

Reviewer sends 15 links to design patterns.
Doesn't explain why they matter here.
Doesn't offer to help.

Junior rewrites code 5 times.
Still doesn't understand why.
Eventually gives up.

THE REALITY:
Code review is teaching, not testing.
The goal is shared understanding.
If they don't learn, you failed.

THE FIX:
1. Explain at their level
   Meet them where they are
   Build up from their understanding

2. Link to learning resources
   "Here's a good explanation: [link]
   Happy to pair on this if helpful."

3. Offer to discuss
   "This is a complex area. Want to
   jump on a call to walk through it?"

4. Choose battles wisely
   Not every PR needs architecture lessons
   Focus on what matters for this change

5. Celebrate learning
   "Great use of X pattern!"
   Positive reinforcement works.

TEACHING FRAMEWORK:
1. What's the issue? (specific)
2. Why does it matter? (impact)
3. What's better? (alternative)
4. How to learn more? (resources)
5. Need help? (offer)

Goal: They can explain this to
the next person.
```

---

## 7. The Premature Approve

**Severity:** High
**Situation:** Approving before CI passes or discussions resolve
**Why Dangerous:** Broken code merges, discussions cut short.

```
THE TRAP:
PR has:
- 3 open discussion threads
- Failing CI pipeline
- No tests for new code

Reviewer:
"Code looks good! Approved."
*Author merges immediately*

Result:
- CI failures now in main
- Discussions never resolved
- Missing tests forgotten
- Technical debt added

THE REALITY:
Approval signals "ready to merge."
If it's not ready, don't approve.

THE FIX:
1. Wait for CI to pass
   Green pipeline = prerequisite
   Don't approve red builds

2. Resolve all threads
   Every discussion should conclude
   "Resolved" or "Won't fix" with reason

3. Verify test coverage
   New code needs new tests
   No tests = not done

4. Use "Approve with comments"
   "LGTM after CI passes"
   "Approve pending test addition"

5. Check merge requirements
   Does your team require:
   - X approvals?
   - Passing CI?
   - Test coverage threshold?
   Don't circumvent them.

APPROVAL CHECKLIST:
â–¡ CI pipeline green
â–¡ All discussions resolved
â–¡ Tests exist for new code
â–¡ Documentation updated if needed
â–¡ No security concerns
â–¡ I would maintain this code

All checked? Then approve.
```

---

## 8. The Asynch-Hole

**Severity:** High
**Situation:** Leaving reviews that require back-and-forth when synchronous would be faster
**Why Dangerous:** Days of comments when 10-minute call would resolve it.

```
THE TRAP:
PR Comment 1: "Not sure about this approach"
Response: "Why not?"
Comment 2: "Well, because X"
Response: "But what about Y?"
Comment 3: "Y doesn't apply because..."
Response: "Can you explain?"
...

3 days of back-and-forth
Could have been 10-minute conversation

THE REALITY:
Async is great for clear feedback.
Async is terrible for complex discussion.
Know when to switch modes.

THE FIX:
1. Recognize complex discussions early
   Multiple valid approaches?
   Architecture decisions?
   â†’ Needs real-time discussion

2. Suggest synchronous when needed
   "This is getting complex. Can we
   do a quick call to align?"

3. Document outcomes
   After call, update PR:
   "Discussed with @reviewer. Agreed
   to approach X because Y."

4. Set timeboxes for async
   If not resolved in 2 round-trips,
   escalate to meeting.

5. Share screen, don't describe
   "Look at line 45..." â†’ Share screen
   Visual is faster than text

ASYNC VS SYNC:
ASYNC (comments):
- Clear issues with obvious fixes
- Style and naming suggestions
- Documentation requests

SYNC (call):
- Design disagreements
- Multiple valid approaches
- Complex explanations
- Repeated back-and-forth
```

---

## 9. The Context Ignorer

**Severity:** High
**Situation:** Reviewing code without understanding the problem it solves
**Why Dangerous:** Wrong feedback, missed issues, frustrated authors.

```
THE TRAP:
PR: "Implement user search feature"

Reviewer (didn't read description):
"Why are you using ElasticSearch here?
Just use a database query."

Author:
"Because we need fuzzy matching,
faceted search, and sub-100ms response
times across 10M records. It's in the
description."

Reviewer: "Oh."

THE REALITY:
Code without context is meaningless.
The "right" solution depends on requirements.
Read the PR description first.

THE FIX:
1. Read description before code
   What problem does this solve?
   What approach was chosen?
   What alternatives were considered?

2. Understand requirements
   What are the constraints?
   Performance needs?
   Scale expectations?

3. Check linked issues/tickets
   Original requirements
   Discussion history
   Design decisions

4. Ask for context if missing
   "Can you add context about why
   this approach was chosen?"

5. Review against requirements
   Does it solve the stated problem?
   Not "is this how I'd do it?"

CONTEXT CHECKLIST:
â–¡ Read PR description
â–¡ Understand problem being solved
â–¡ Check linked issues
â–¡ Know constraints/requirements
â–¡ Understand chosen approach reasoning

THEN review the code.
```

---

## 10. The Ghost Reviewer

**Severity:** High
**Situation:** Assigned as reviewer but never responds
**Why Dangerous:** PRs age, authors blocked, resentment builds.

```
THE TRAP:
PR assigned Tuesday.
Ping Thursday: "Hey, any update?"
Ping Monday: "Still waiting..."
Ping next Thursday: "?"

Two weeks later:
"Oh sorry, been busy. Looking now."

*Requests major changes*

Author: Has already moved on to other work

THE REALITY:
Review delay is team delay.
Author is blocked.
Context is lost.
Momentum dies.

THE FIX:
1. Set review SLAs
   Review within 24 hours (business)
   Or explicitly decline

2. If too busy, say so immediately
   "Can't review this week.
   Please reassign to @other"

3. Partial reviews > no reviews
   "Looked at auth changes (LGTM).
   Will review API changes tomorrow."

4. Use review request properly
   Don't accept if you can't do it
   Reassign quickly if blocked

5. Automate reminders
   PR older than 24h â†’ Slack reminder
   Make aging visible

REVIEWER RESPONSIBILITIES:
- Respond within SLA
- If blocked, communicate immediately
- If need reassignment, arrange it
- If doing later, commit to timeframe

"I'll get to it" is not a commitment.
"I'll review by EOD Thursday" is.
```

---

## 11. The Approval Hostage

**Severity:** High
**Situation:** Blocking merge for personal preferences, not actual issues
**Why Dangerous:** Personal taste becomes law, velocity dies.

```
THE TRAP:
PR is:
- Functionally correct
- Has tests
- Follows standards
- CI passing

Reviewer blocks:
"I prefer a different variable name"
"I would have done this differently"
"This isn't how we usually do it"

Author:
"But it works and meets requirements..."

Reviewer:
"I just don't like it. Not approving."

PR blocked for personal preference.

THE REALITY:
Your preferences aren't requirements.
Working code that meets standards = good.
Perfect is the enemy of shipped.

THE FIX:
1. Distinguish blocking from preference
   BLOCKING: Will break, is insecure, violates standard
   PREFERENCE: I would do it differently

2. Mark preferences as non-blocking
   "nit: I'd prefer X, but fine either way"
   "suggestion: Consider Y, not blocking"

3. Accept multiple valid solutions
   Your way isn't the only way
   Working code > your preferences

4. Define what's actually blocking
   Security issues
   Clear bugs
   Standard violations
   Missing requirements

5. Know when to let go
   "I'd do it differently, but this works.
   Approved."

BLOCKING VS PREFERENCE:
BLOCKING:
- Security vulnerability
- Will cause bugs
- Breaks existing functionality
- Missing requirements
- Violates team standards

NON-BLOCKING:
- Style preferences
- Alternative approaches
- Different naming
- "I would have..."

If it's not in the blocking list,
don't block the merge.
```

---

## 12. The Missing Security Eye

**Severity:** Critical
**Situation:** Reviewing code without checking for security issues
**Why Dangerous:** Vulnerabilities ship to production.

```
THE TRAP:
PR Review:
âœ“ "Logic looks correct"
âœ“ "Good naming"
âœ“ "Tests pass"
*Approved*

Shipped code contains:
- SQL injection in line 23
- XSS vulnerability in line 45
- Hardcoded credentials in config
- Missing auth check on endpoint

"But I reviewed the code!"
Yes, but not for security.

THE REALITY:
Security review is code review.
Every PR is a potential vulnerability.
If you're not checking, who is?

THE FIX:
1. Security checklist per PR
   â–¡ Input validation
   â–¡ Output encoding
   â–¡ Authentication required
   â–¡ Authorization checked
   â–¡ No secrets in code
   â–¡ SQL parameterized
   â–¡ Dependencies vetted

2. Know OWASP Top 10
   SQL injection
   XSS
   Broken authentication
   Sensitive data exposure
   etc.

3. Check common patterns
   User input â†’ database?
   User input â†’ HTML output?
   User input â†’ system command?
   External data â†’ trust?

4. Use automated tools
   Snyk, Dependabot, CodeQL
   But don't rely on them alone

5. When in doubt, ask security team
   "This handles user input. Can
   security team take a look?"

SECURITY REVIEW MINIMUM:
â–¡ Auth/authz on all endpoints
â–¡ Input validated before use
â–¡ No SQL string concatenation
â–¡ No eval() with user data
â–¡ No secrets in code
â–¡ Dependencies up to date
â–¡ Sensitive data handled properly
```

## Decision Framework

# Decisions: Code Review

Critical decisions that determine review effectiveness and team velocity.

---

## Decision 1: Review Scope

**Context:** Deciding what should be reviewed and how deeply.

**Options:**

| Scope | Description | Pros | Cons |
|-------|-------------|------|------|
| **All code** | Every line reviewed | Complete coverage | Slow, bottleneck |
| **Changed code only** | Only diff reviewed | Fast | Miss context |
| **Risk-based** | Depth varies by risk | Efficient | Requires judgment |
| **Sample-based** | Random sampling | Fast | Gaps in coverage |

**Framework:**
```
Review scope by code type:

ALWAYS DEEP REVIEW:
- Security-sensitive code
- Authentication/Authorization
- Payment processing
- Data handling (PII)
- Public APIs
- Database migrations

STANDARD REVIEW:
- Feature code
- Business logic
- API endpoints
- State management
- Error handling

LIGHT REVIEW:
- Tests (review but less depth)
- Documentation
- Configuration
- Styling changes

TRUST-BASED SKIP:
- Auto-generated code
- Dependency updates (automated)
- Typo fixes
- Formatting changes

DEPTH LEVELS:
DEEP: Read every line, test mentally
STANDARD: Understand logic, check patterns
LIGHT: Skim for obvious issues
VERIFY: Confirm change matches description

RISK ASSESSMENT:
What breaks if this fails?
- User data exposed â†’ Deep
- Feature doesn't work â†’ Standard
- Formatting off â†’ Light

WHO'S AUTHORING:
- New team member â†’ Deeper
- Senior on familiar code â†’ Standard
- Anyone on unfamiliar code â†’ Deeper
```

**Default Recommendation:** Risk-based review. Deep for security/data, standard for features, light for tests/docs.

---

## Decision 2: Number of Reviewers

**Context:** How many people should review each PR.

**Options:**

| Count | When | Pros | Cons |
|-------|------|------|------|
| **1** | Simple changes | Fast | Single point of failure |
| **2** | Standard work | Good coverage | Slower |
| **3+** | Critical code | Maximum coverage | Bottleneck |
| **CODEOWNERS** | Auto-assigned | Domain expertise | Complex setup |

**Framework:**
```
Reviewer count by PR type:

ONE REVIEWER:
- Bug fixes (non-critical)
- Documentation updates
- Minor features
- Styling changes
- Test additions
- Config changes

TWO REVIEWERS:
- New features
- API changes
- Database changes
- Cross-team changes
- Refactoring
- Performance changes

THREE+ REVIEWERS:
- Security changes
- Architecture changes
- Breaking changes
- Production hotfixes
- Core system changes

REQUIRED REVIEWERS:
Security changes â†’ Security team member
Database â†’ DBA or senior backend
API â†’ API owner
Frontend â†’ Frontend lead
Breaking change â†’ Tech lead

CODEOWNERS STRATEGY:
/src/auth/** @security-team
/src/payments/** @payments-team @security-team
/api/** @api-team
/.github/** @platform-team

REVIEWER ASSIGNMENT:
Primary: Domain expert
Secondary: Different perspective
Tertiary: (for critical) Senior/lead

DON'T:
- Require 5 reviewers for everything
- Let anyone review anything
- Skip required reviewers for speed
```

**Default Recommendation:** 1 for simple, 2 for standard features, 2+ with required reviewers for critical paths.

---

## Decision 3: Review Turnaround Time

**Context:** Setting expectations for review response time.

**Options:**

| SLA | Target | Pros | Cons |
|-----|--------|------|------|
| **Same day** | < 8 hours | Fast iteration | May be shallow |
| **Next day** | < 24 hours | Balanced | Slower velocity |
| **48 hours** | < 48 hours | Thorough | Blocks authors |
| **ASAP for critical** | < 2 hours | Urgent handled | Disruptive |

**Framework:**
```
Review SLA by PR type:

URGENT (< 2 hours):
- Production hotfixes
- Security patches
- Critical bug fixes
Signal: "urgent" label, direct message

PRIORITY (< 4 hours):
- Release blockers
- User-facing bugs
- Dependency on other work
Signal: "priority" label

STANDARD (< 24 hours):
- Normal features
- Non-urgent bugs
- Improvements
Default for most PRs

BATCH (48-72 hours):
- Large refactors
- Non-urgent improvements
- Documentation
Signal: "no-rush" label

SLA EXPECTATIONS:
Initial response: Within SLA
Full review: May take longer
Follow-up: Same day after author response

COMMUNICATION:
"Can't review today. Will look tomorrow."
"Started review, will finish this afternoon."
"Complex PR, need 2 hours to review properly."

SLA BREAKERS:
If you can't meet SLA:
- Notify author immediately
- Suggest alternate reviewer
- Commit to new timeline

TRACKING:
Measure: Time from request to first response
Target: 90% within SLA
Review: Monthly metrics
```

**Default Recommendation:** 24-hour SLA for standard PRs, same-day for priority, 2-hour for urgent. Track and report.

---

## Decision 4: Review Approval Requirements

**Context:** What's needed to merge a PR.

**Options:**

| Requirement | Description | Pros | Cons |
|-------------|-------------|------|------|
| **1 approval** | Single LGTM | Fast | Less coverage |
| **2 approvals** | Two LGTMs | Better coverage | Slower |
| **Passing CI** | Required green | Automated quality | May slow down |
| **All resolved** | Discussions closed | Complete review | Can bottleneck |

**Framework:**
```
Merge requirements by branch:

MAIN/PRODUCTION:
- 2 approvals (1 from CODEOWNER)
- CI passing
- All conversations resolved
- No "Request Changes"
- Security scan passed

DEVELOP/STAGING:
- 1 approval
- CI passing
- Critical conversations resolved
- Security scan passed

FEATURE BRANCHES:
- 1 approval
- Tests passing
- Linting passing

RELEASE BRANCHES:
- 2 approvals
- Full CI pipeline
- QA sign-off
- Security sign-off

HOTFIX FLOW:
- 1 approval from senior
- Critical tests passing
- Post-merge full test
- Follow-up PR for full tests

APPROVAL MEANINGS:
â˜ Comment: Feedback, not blocking
â˜‘ Approve: Ready to merge
â˜’ Request Changes: Don't merge

RESOLVED CONVERSATIONS:
- Author addressed or explained
- Reviewer confirmed resolution
- Won't fix documented with reason

AUTO-MERGE RULES:
Enable for:
- Approved PRs
- All checks passed
- No unresolved conversations
```

**Default Recommendation:** 2 approvals + passing CI + resolved conversations for main. 1 approval for feature branches.

---

## Decision 5: PR Size Limits

**Context:** Managing PR size for effective review.

**Options:**

| Size | Lines Changed | Pros | Cons |
|------|---------------|------|------|
| **Small** | < 100 lines | Easy review | More PRs |
| **Medium** | 100-400 lines | Balanced | Some complexity |
| **Large** | 400-1000 lines | Complete features | Hard to review |
| **XL** | 1000+ lines | Rare, complex | Nearly impossible |

**Framework:**
```
PR size recommendations:

IDEAL SIZE:
100-400 lines changed
Single logical change
Reviewable in 30-60 minutes

SIZE LIMITS:
< 50 lines: Quick review
50-200 lines: Standard review
200-400 lines: Extended review
400-800 lines: Consider splitting
800+ lines: Should split

LARGE PR HANDLING:
If large PR unavoidable:
1. Add summary/walkthrough
2. Suggest review order
3. Mark key areas
4. Offer pair review
5. Allow staged approval

SPLITTING STRATEGIES:

VERTICAL SLICE:
One feature end-to-end
Data â†’ Logic â†’ UI

HORIZONTAL LAYER:
Infrastructure first
Business logic second
UI third

PREP + FEATURE:
Refactoring PR first
Feature PR after

FEATURE FLAGS:
Ship incomplete behind flag
Multiple small PRs
Flag enabled when done

EXCEPTIONS:
Large PRs acceptable for:
- Auto-generated code
- Bulk migrations
- Major refactors (with discussion)
- Initial project setup

TRACKING:
Measure average PR size
Target: 80% under 400 lines
Review large PR reasons
```

**Default Recommendation:** Target 100-400 lines. Require explanation for 400+. Block 1000+ without exception approval.

---

## Decision 6: Review Automation

**Context:** What to automate vs. human review.

**Options:**

| Automation | Handles | Pros | Cons |
|------------|---------|------|------|
| **Linting** | Style | Fast, consistent | Setup required |
| **Type checking** | Types | Catch type errors | Build time |
| **Tests** | Logic | Verify behavior | Coverage varies |
| **Security scan** | Vulnerabilities | Consistent | False positives |

**Framework:**
```
Automation strategy:

AUTOMATE (No human needed):
- Formatting (Prettier)
- Linting (ESLint)
- Type checking (TypeScript)
- Test execution
- Security scanning
- Dependency updates
- Coverage thresholds
- Build verification

ASSIST (Human + machine):
- Complexity analysis
- Code duplication
- Pattern suggestions
- PR description check
- Review reminders

HUMAN REQUIRED:
- Logic correctness
- Architecture decisions
- Business requirements
- Edge case coverage
- Code clarity
- Naming quality
- Design patterns

TOOL STACK:
// .github/workflows/pr-checks.yml
jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - run: npm run lint

  typecheck:
    runs-on: ubuntu-latest
    steps:
      - run: npm run type-check

  test:
    runs-on: ubuntu-latest
    steps:
      - run: npm test -- --coverage

  security:
    runs-on: ubuntu-latest
    steps:
      - run: npm audit
      - uses: snyk/actions/node@master

AUTOMATION RULES:
- Fail fast (lint before test)
- Block merge on failure
- No human override for security
- Allow override for coverage (with comment)

DON'T AUTOMATE:
- "Is this the right approach?"
- "Is this maintainable?"
- "Does this solve the problem?"
```

**Default Recommendation:** Automate style, types, tests, security. Require human for logic, architecture, completeness.

---

## Decision 7: Feedback Style Guidelines

**Context:** How to write review comments.

**Options:**

| Style | Approach | Pros | Cons |
|-------|----------|------|------|
| **Direct** | Clear, brief | Efficient | May feel harsh |
| **Socratic** | Questions | Learning | Slower |
| **Sandwich** | Positive first | Soft | Can be insincere |
| **Tiered** | Labeled severity | Clear priority | Extra work |

**Framework:**
```
Feedback style guide:

COMMENT STRUCTURE:
[TIER] What + Why + How

TIER PREFIXES:
ðŸ”´ BLOCKER: Must fix before merge
ðŸŸ¡ ISSUE: Should fix
ðŸŸ¢ SUGGESTION: Consider
ðŸ’¬ NIT: Optional, minor
âœ¨ NICE: Positive callout

EXAMPLE:
ðŸŸ¡ ISSUE: This could fail with empty array.
The `[0]` access will throw if `items`
is empty. Consider adding a guard check:
`if (!items.length) return null`

TONE:
- "Consider..." not "You should..."
- "This might..." not "This will..."
- "We could..." not "You need to..."
- Ask questions when uncertain

POSITIVE FEEDBACK:
Required, not optional.
Every PR should have something positive.
"Nice test coverage"
"Clean separation of concerns"
"Good error handling"

HARSH VS DIRECT:
HARSH: "This is wrong"
DIRECT: "This has a bug: X case fails"

HARSH: "Why would you do this?"
DIRECT: "What led to this approach?
I'm curious about the Y trade-off."

THREADING:
- Start new thread for new topic
- Keep discussion in original thread
- Resolve when addressed

TIMING:
Give all feedback at once
Don't drip-feed comments
Author can batch responses
```

**Default Recommendation:** Tiered comments with direct-but-kind tone. Always include positive feedback.

---

## Decision 8: Handling Disagreements

**Context:** Resolving conflicts between reviewer and author.

**Options:**

| Approach | When | Pros | Cons |
|----------|------|------|------|
| **Author decides** | Opinion matters | Fast | Standards may slip |
| **Reviewer decides** | Blocker | Quality maintained | May frustrate |
| **Tech lead decides** | Escalation | Authoritative | Bottleneck |
| **Team discussion** | Architecture | Shared understanding | Slow |

**Framework:**
```
Disagreement resolution:

SEVERITY-BASED:

BLOCKING ISSUES (Reviewer wins):
- Security vulnerabilities
- Bugs that will break production
- Standard violations
Author must fix or explain why not.

SUGGESTIONS (Author decides):
- Style preferences
- Alternative approaches
- Optional improvements
Author can accept or decline.

DESIGN DISAGREEMENTS (Escalate):
- Architecture decisions
- Trade-off debates
- Significant divergence
Bring in third party.

RESOLUTION PROCESS:

Step 1: Clarify positions
"Help me understand why you chose X"
"My concern is that Y will happen"

Step 2: Find common ground
"We both want Z, right?"
"The disagreement is about approach"

Step 3: Evaluate trade-offs
"X gives us A but costs B"
"Y gives us C but costs D"

Step 4: Decide or escalate
If consensus: Document and proceed
If not: Bring in third party

ESCALATION LADDER:
1. Peer discussion (1 round)
2. Senior/tech lead (1 opinion)
3. Team discussion (if needed)
4. Tech lead decision (final)

TIME-BOX:
Max 2 async back-and-forth
Then: Call or escalate
Don't let PRs rot in debate

DOCUMENTATION:
Document decision in PR
"Discussed with @reviewer. Decided to
proceed with X because Y. See thread."
```

**Default Recommendation:** Author decides on suggestions, reviewer decides on blockers, escalate design disagreements quickly.

---

## Decision 9: Review Focus Areas

**Context:** What to prioritize during review.

**Options:**

| Focus | Priority | When |
|-------|----------|------|
| **Correctness** | Always | All PRs |
| **Security** | High | All PRs |
| **Performance** | Medium | Data/scale changes |
| **Style** | Low | Automated |
| **Readability** | Medium | All PRs |

**Framework:**
```
Review priority checklist:

ALWAYS CHECK (Every PR):
1. Does it work correctly?
2. Is it secure?
3. Does it have tests?
4. Is it readable?
5. Does it match requirements?

CHECK FOR SPECIFIC CHANGES:

DATABASE CHANGES:
- Migration reversibility
- Index performance
- Data integrity
- Deadlock potential

API CHANGES:
- Breaking changes
- Documentation
- Error responses
- Rate limiting

AUTHENTICATION:
- Session handling
- Token security
- Permission checks
- Logout behavior

PERFORMANCE CRITICAL:
- Algorithm complexity
- Memory usage
- Database queries
- Caching strategy

UI CHANGES:
- Accessibility
- Responsive design
- Error states
- Loading states

FOCUS ORDER:
1. Blockers (security, bugs)
2. Logic (correctness)
3. Testing (coverage, quality)
4. Design (maintainability)
5. Style (let tools handle)

TIME ALLOCATION:
60%: Logic and correctness
20%: Security and edge cases
15%: Design and maintainability
5%: Style and naming

DON'T FOCUS ON:
- Formatting (automated)
- Import order (automated)
- Spacing (automated)
```

**Default Recommendation:** Correctness > Security > Testing > Maintainability > Style. Let tools handle style.

---

## Decision 10: Handling Legacy Code Changes

**Context:** Reviewing changes to old or problematic code.

**Options:**

| Approach | Rule | Pros | Cons |
|----------|------|------|------|
| **Boy scout** | Leave better | Gradual improvement | Scope creep |
| **Minimal touch** | Change only needed | Focused | Debt remains |
| **Big bang** | Full refactor | Clean code | Risky, slow |
| **Document debt** | Track issues | Awareness | No improvement |

**Framework:**
```
Legacy code review policy:

THE QUESTION:
PR touches legacy code.
Should we require cleanup?

MINIMAL TOUCH (Default):
PR does X.
Reviewer evaluates X.
Don't require unrelated cleanup.

BOY SCOUT (Optional):
"While you're here, could you..."
Small improvements welcome.
Don't mandate.

WHEN TO ENCOURAGE CLEANUP:
- Obvious bug nearby
- Clear security issue
- Simple improvement
- Related to change

WHEN NOT TO EXPAND SCOPE:
- Major refactoring needed
- Different responsibility
- Time-sensitive change
- Would need new tests

LEGACY CODE RULES:

FOR AUTHOR:
- Don't refactor while fixing
- Separate PRs for cleanup
- Note tech debt discovered
- "Found issue X, created ticket Y"

FOR REVIEWER:
- Review the change, not the file
- Don't block for existing issues
- Create tickets for future work
- "This file has issues. Created #456
  for future cleanup. This PR is fine."

TECH DEBT HANDLING:
1. Note in review (don't block)
2. Create tracking ticket
3. Add to backlog
4. Prioritize separately

EXAMPLE:
PR: Fix date parsing bug

Reviewer sees: 500 lines of messy code

DON'T: "Refactor this entire file"
DO: "Bug fix looks good. Created ticket
#789 for the broader file cleanup."

EXCEPTIONS:
Require cleanup if:
- Security vulnerability
- Will cause bug
- Can't understand change otherwise
```

**Default Recommendation:** Minimal touch for legacy. Create tickets for tech debt. Don't scope-creep PRs.

## Collaboration

### When to Hand Off

| Trigger | Delegate To | Context |
|---------|-------------|--------|
| `security vulnerability|penetration|exploit` | cybersecurity | Code review found security issue |
| `test coverage|testing strategy` | qa-engineering | Code review found testing gaps |
| `performance|optimization|refactor` | codebase-optimization | Code review found optimization opportunity |

### Receives Work From

- **frontend**: Frontend code needs review
- **backend**: Backend code needs review
- **devops**: Infrastructure code needs review
- **cybersecurity**: Security-sensitive code needs review

### Works Well With

- frontend
- backend
- qa-engineering
- cybersecurity
- codebase-optimization
- devops

---

## Get the Full Version

This skill has **automated validations**, **detection patterns**, and **structured handoff triggers** that work with the Spawner orchestrator.

```bash
npx vibeship-spawner-skills install
```

Full skill path: `~/.spawner/skills/development/code-review/`

**Includes:**
- `skill.yaml` - Structured skill definition
- `sharp-edges.yaml` - Machine-parseable gotchas with detection patterns
- `validations.yaml` - Automated code checks
- `collaboration.yaml` - Handoff triggers for skill orchestration

**Deep content:**
- `patterns.md` - Comprehensive pattern library
- `anti-patterns.md` - What to avoid and why
- `sharp-edges.md` - Detailed gotcha documentation
- `decisions.md` - Decision frameworks

---

*Generated by [VibeShip Spawner](https://github.com/vibeforge1111/vibeship-spawner-skills)*
