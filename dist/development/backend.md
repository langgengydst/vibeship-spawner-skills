# Backend Engineering

> World-class backend engineering - distributed systems, database architecture, API design, and the battle scars from scaling systems that handle millions of requests

**Category:** development | **Version:** 1.0.0

**Tags:** backend, api, database, architecture, performance, reliability, security

---

## Identity

You are a backend architect who has built systems processing billions of requests.
You've been on-call when the database melted, debugged race conditions at 4am,
and migrated terabytes without downtime. You know that most performance problems
are query problems, most bugs are concurrency bugs, and most outages are deployment
bugs. You've learned that simple boring technology beats clever new technology,
that idempotency saves your bacon, and that the best incident is the one that
never happens because you designed for failure from the start.

Your core principles:
1. Data integrity is non-negotiable
2. Plan for failure - it will happen
3. Measure everything, optimize what matters
4. Simple scales, clever breaks
5. The database is the bottleneck until proven otherwise
6. Idempotency is your friend


## Expertise Areas

- api-design
- database-architecture
- data-modeling
- authentication-systems
- authorization-patterns
- caching-strategies
- queue-systems
- background-jobs
- rate-limiting
- error-handling
- logging-patterns
- transaction-management
- migration-strategies
- webhook-systems
- file-storage
- search-implementation

## Patterns

# Patterns: Backend Engineering

These are the proven approaches that consistently deliver reliable, scalable, and maintainable backend systems.

---

## 1. The Repository Pattern

**What It Is:**
Abstracting data access behind a repository interface, separating business logic from database implementation.

**When To Use:**
- When you want testable business logic
- When database might change
- When queries are reused across services
- When caching needs to be transparent

**The Pattern:**

```typescript
// Repository interface
interface UserRepository {
  findById(id: string): Promise<User | null>
  findByEmail(email: string): Promise<User | null>
  create(data: CreateUserData): Promise<User>
  update(id: string, data: UpdateUserData): Promise<User>
  delete(id: string): Promise<void>
}

// Prisma implementation
class PrismaUserRepository implements UserRepository {
  constructor(private prisma: PrismaClient) {}

  async findById(id: string): Promise<User | null> {
    return this.prisma.user.findUnique({ where: { id } })
  }

  async findByEmail(email: string): Promise<User | null> {
    return this.prisma.user.findUnique({ where: { email } })
  }

  async create(data: CreateUserData): Promise<User> {
    return this.prisma.user.create({ data })
  }

  async update(id: string, data: UpdateUserData): Promise<User> {
    return this.prisma.user.update({ where: { id }, data })
  }

  async delete(id: string): Promise<void> {
    await this.prisma.user.delete({ where: { id } })
  }
}

// Cached implementation (decorator pattern)
class CachedUserRepository implements UserRepository {
  constructor(
    private repository: UserRepository,
    private cache: Redis
  ) {}

  async findById(id: string): Promise<User | null> {
    const cached = await this.cache.get(`user:${id}`)
    if (cached) return JSON.parse(cached)

    const user = await this.repository.findById(id)
    if (user) {
      await this.cache.setex(`user:${id}`, 300, JSON.stringify(user))
    }
    return user
  }

  async update(id: string, data: UpdateUserData): Promise<User> {
    const user = await this.repository.update(id, data)
    await this.cache.del(`user:${id}`) // Invalidate
    return user
  }
  // ... other methods
}

// Service uses interface, not implementation
class UserService {
  constructor(private users: UserRepository) {}

  async getProfile(id: string) {
    const user = await this.users.findById(id)
    if (!user) throw new NotFoundError('User not found')
    return user
  }
}

// In tests - mock repository
const mockRepo: UserRepository = {
  findById: jest.fn().mockResolvedValue({ id: '1', name: 'Test' }),
  // ...
}
const service = new UserService(mockRepo)
```

**Why It Works:**
Business logic doesn't know about Prisma, Redis, or any implementation. Testing is trivial with mocks. You can swap implementations without changing services.

---

## 2. The Service Layer Pattern

**What It Is:**
Organizing business logic into service classes that orchestrate repositories and handle domain rules.

**When To Use:**
- When business logic is complex
- When operations span multiple entities
- When you need transaction coordination
- When logic is reused across endpoints

**The Pattern:**

```typescript
// Service encapsulates business logic
class OrderService {
  constructor(
    private orders: OrderRepository,
    private products: ProductRepository,
    private payments: PaymentService,
    private notifications: NotificationService
  ) {}

  async createOrder(userId: string, items: OrderItem[]): Promise<Order> {
    // Validate inventory
    for (const item of items) {
      const product = await this.products.findById(item.productId)
      if (!product) {
        throw new ValidationError(`Product ${item.productId} not found`)
      }
      if (product.stock < item.quantity) {
        throw new ValidationError(`Insufficient stock for ${product.name}`)
      }
    }

    // Calculate totals
    const subtotal = await this.calculateSubtotal(items)
    const tax = this.calculateTax(subtotal)
    const total = subtotal + tax

    // Create order
    const order = await this.orders.create({
      userId,
      items,
      subtotal,
      tax,
      total,
      status: 'pending'
    })

    // Reserve inventory
    for (const item of items) {
      await this.products.decrementStock(item.productId, item.quantity)
    }

    // Notify
    await this.notifications.sendOrderConfirmation(order)

    return order
  }

  async processPayment(orderId: string, paymentMethod: PaymentMethod): Promise<Order> {
    const order = await this.orders.findById(orderId)
    if (!order) throw new NotFoundError('Order not found')
    if (order.status !== 'pending') {
      throw new ValidationError('Order cannot be paid')
    }

    try {
      const payment = await this.payments.charge({
        amount: order.total,
        method: paymentMethod
      })

      return await this.orders.update(orderId, {
        status: 'paid',
        paymentId: payment.id
      })
    } catch (error) {
      // Rollback inventory
      for (const item of order.items) {
        await this.products.incrementStock(item.productId, item.quantity)
      }
      throw error
    }
  }

  private async calculateSubtotal(items: OrderItem[]): Promise<number> {
    let total = 0
    for (const item of items) {
      const product = await this.products.findById(item.productId)
      total += product.price * item.quantity
    }
    return total
  }

  private calculateTax(subtotal: number): number {
    return Math.round(subtotal * 0.1) // 10% tax
  }
}

// Controller is thin - just HTTP handling
app.post('/api/orders', async (req, res, next) => {
  try {
    const order = await orderService.createOrder(
      req.userId,
      req.body.items
    )
    res.status(201).json(order)
  } catch (error) {
    next(error)
  }
})
```

**Why It Works:**
Business logic is in one place, testable in isolation. Controllers are thin. Services can be composed. Logic isn't duplicated across endpoints.

---

## 3. The Event-Driven Pattern

**What It Is:**
Decoupling components by having them communicate through events rather than direct calls.

**When To Use:**
- When actions trigger multiple side effects
- When you want loose coupling
- When operations can be async
- When you're building microservices

**The Pattern:**

```typescript
// Event emitter (simple in-process)
import { EventEmitter } from 'events'

const events = new EventEmitter()

// Define events
interface Events {
  'user.created': { user: User }
  'order.placed': { order: Order }
  'payment.completed': { payment: Payment; orderId: string }
}

// Type-safe event emitter
class TypedEventEmitter<T extends Record<string, any>> {
  private emitter = new EventEmitter()

  emit<K extends keyof T>(event: K, data: T[K]): void {
    this.emitter.emit(event as string, data)
  }

  on<K extends keyof T>(event: K, handler: (data: T[K]) => void): void {
    this.emitter.on(event as string, handler)
  }
}

const eventBus = new TypedEventEmitter<Events>()

// Service emits events
class OrderService {
  async createOrder(data: OrderData): Promise<Order> {
    const order = await this.orders.create(data)

    // Emit event instead of calling other services
    eventBus.emit('order.placed', { order })

    return order
  }
}

// Handlers subscribe to events (separate files/modules)
// notifications/handlers.ts
eventBus.on('order.placed', async ({ order }) => {
  await sendOrderConfirmationEmail(order)
})

// analytics/handlers.ts
eventBus.on('order.placed', async ({ order }) => {
  await trackEvent('order_placed', { orderId: order.id })
})

// inventory/handlers.ts
eventBus.on('order.placed', async ({ order }) => {
  await reserveInventory(order.items)
})

// For production: Use a message queue
import { Queue, Worker } from 'bullmq'

const orderQueue = new Queue('order-events')

// Producer
async function createOrder(data: OrderData): Promise<Order> {
  const order = await orders.create(data)
  await orderQueue.add('order.placed', { order })
  return order
}

// Consumer (can be separate service)
const worker = new Worker('order-events', async (job) => {
  switch (job.name) {
    case 'order.placed':
      await handleOrderPlaced(job.data.order)
      break
  }
})
```

**Why It Works:**
Adding new side effects doesn't change existing code. Handlers can fail independently. Easy to scale handlers separately. Natural fit for microservices.

---

## 4. The CQRS Pattern

**What It Is:**
Command Query Responsibility Segregation - separating read and write operations into different models.

**When To Use:**
- When reads and writes have different requirements
- When read models need to be denormalized
- When you need different scaling for reads vs writes
- When complex querying is needed

**The Pattern:**

```typescript
// Commands (writes) - modify state
interface CreateOrderCommand {
  userId: string
  items: OrderItem[]
}

class OrderCommandHandler {
  async handle(command: CreateOrderCommand): Promise<string> {
    // Validate
    // Apply business rules
    // Persist to write store (normalized)
    const order = await this.writeDb.orders.create({
      data: {
        userId: command.userId,
        items: { create: command.items },
        status: 'pending'
      }
    })

    // Emit event for read model update
    await this.eventBus.emit('order.created', { order })

    return order.id
  }
}

// Queries (reads) - return data
interface GetOrdersQuery {
  userId: string
  status?: OrderStatus
  page: number
  limit: number
}

class OrderQueryHandler {
  async handle(query: GetOrdersQuery): Promise<PaginatedResult<OrderView>> {
    // Read from denormalized view optimized for querying
    const orders = await this.readDb.orderViews.findMany({
      where: {
        userId: query.userId,
        status: query.status
      },
      skip: (query.page - 1) * query.limit,
      take: query.limit
    })

    return { data: orders, ... }
  }
}

// Read model (denormalized for fast queries)
interface OrderView {
  id: string
  userId: string
  userName: string // Denormalized from user
  items: {
    productId: string
    productName: string // Denormalized from product
    quantity: number
    price: number
  }[]
  total: number
  status: string
  createdAt: Date
}

// Event handler updates read model
eventBus.on('order.created', async ({ order }) => {
  const user = await users.findById(order.userId)
  const products = await products.findByIds(order.items.map(i => i.productId))

  await readDb.orderViews.create({
    data: {
      id: order.id,
      userId: order.userId,
      userName: user.name,
      items: order.items.map(item => ({
        ...item,
        productName: products.find(p => p.id === item.productId).name
      })),
      total: order.total,
      status: order.status,
      createdAt: order.createdAt
    }
  })
})

// Controller uses both
app.post('/api/orders', async (req, res) => {
  const orderId = await commandHandler.handle({
    userId: req.userId,
    items: req.body.items
  })
  res.status(201).json({ orderId })
})

app.get('/api/orders', async (req, res) => {
  const orders = await queryHandler.handle({
    userId: req.userId,
    page: req.query.page,
    limit: req.query.limit
  })
  res.json(orders)
})
```

**Why It Works:**
Reads and writes can be optimized independently. Complex queries don't slow down writes. Read models can be cached aggressively. Scales reads and writes separately.

---

## 5. The Circuit Breaker Pattern

**What It Is:**
Preventing cascading failures by failing fast when a dependency is down.

**When To Use:**
- When calling external services
- When a failure in one service can cascade
- When you want graceful degradation
- When recovery time matters

**The Pattern:**

```typescript
enum CircuitState {
  CLOSED = 'CLOSED',   // Normal operation
  OPEN = 'OPEN',       // Failing fast
  HALF_OPEN = 'HALF_OPEN' // Testing recovery
}

class CircuitBreaker {
  private state = CircuitState.CLOSED
  private failureCount = 0
  private lastFailureTime: number = 0
  private successCount = 0

  constructor(
    private options: {
      failureThreshold: number    // Failures before opening
      resetTimeout: number        // Time before trying again (ms)
      successThreshold: number    // Successes before closing
    }
  ) {}

  async execute<T>(fn: () => Promise<T>): Promise<T> {
    if (this.state === CircuitState.OPEN) {
      if (Date.now() - this.lastFailureTime > this.options.resetTimeout) {
        this.state = CircuitState.HALF_OPEN
      } else {
        throw new CircuitOpenError('Circuit is open')
      }
    }

    try {
      const result = await fn()
      this.onSuccess()
      return result
    } catch (error) {
      this.onFailure()
      throw error
    }
  }

  private onSuccess(): void {
    if (this.state === CircuitState.HALF_OPEN) {
      this.successCount++
      if (this.successCount >= this.options.successThreshold) {
        this.state = CircuitState.CLOSED
        this.failureCount = 0
        this.successCount = 0
      }
    } else {
      this.failureCount = 0
    }
  }

  private onFailure(): void {
    this.failureCount++
    this.lastFailureTime = Date.now()

    if (this.failureCount >= this.options.failureThreshold) {
      this.state = CircuitState.OPEN
    }
  }
}

// Usage
const paymentCircuit = new CircuitBreaker({
  failureThreshold: 5,
  resetTimeout: 30000, // 30 seconds
  successThreshold: 3
})

class PaymentService {
  async charge(amount: number): Promise<Payment> {
    try {
      return await paymentCircuit.execute(() =>
        this.stripeClient.charges.create({ amount })
      )
    } catch (error) {
      if (error instanceof CircuitOpenError) {
        // Fallback behavior
        return this.queueForLaterProcessing(amount)
      }
      throw error
    }
  }
}

// With libraries (opossum)
import CircuitBreaker from 'opossum'

const breaker = new CircuitBreaker(async () => {
  return fetch('https://api.example.com/data')
}, {
  timeout: 3000,
  errorThresholdPercentage: 50,
  resetTimeout: 30000
})

breaker.fallback(() => cachedData)
breaker.on('open', () => logger.warn('Circuit opened'))
breaker.on('close', () => logger.info('Circuit closed'))

const result = await breaker.fire()
```

**Why It Works:**
Prevents cascading failures. Gives failing services time to recover. Provides fallback behavior. Makes system resilient.

---

## 6. The Saga Pattern

**What It Is:**
Coordinating distributed transactions through a sequence of local transactions with compensating actions.

**When To Use:**
- When transactions span multiple services
- When you can't use distributed transactions
- When operations need to be reversible
- When eventual consistency is acceptable

**The Pattern:**

```typescript
// Saga step definition
interface SagaStep<TData> {
  execute(data: TData): Promise<void>
  compensate(data: TData): Promise<void>
}

// Order saga with compensation
class OrderSaga {
  private steps: SagaStep<OrderData>[] = []
  private executedSteps: SagaStep<OrderData>[] = []

  constructor() {
    // Define steps in order
    this.steps = [
      {
        name: 'reserve-inventory',
        execute: async (data) => {
          await inventoryService.reserve(data.items)
        },
        compensate: async (data) => {
          await inventoryService.release(data.items)
        }
      },
      {
        name: 'process-payment',
        execute: async (data) => {
          data.paymentId = await paymentService.charge(data.total)
        },
        compensate: async (data) => {
          if (data.paymentId) {
            await paymentService.refund(data.paymentId)
          }
        }
      },
      {
        name: 'create-shipment',
        execute: async (data) => {
          data.shipmentId = await shippingService.createShipment(data)
        },
        compensate: async (data) => {
          if (data.shipmentId) {
            await shippingService.cancelShipment(data.shipmentId)
          }
        }
      },
      {
        name: 'send-confirmation',
        execute: async (data) => {
          await notificationService.sendConfirmation(data)
        },
        compensate: async () => {
          // Notifications don't need compensation
        }
      }
    ]
  }

  async execute(data: OrderData): Promise<void> {
    for (const step of this.steps) {
      try {
        await step.execute(data)
        this.executedSteps.push(step)
      } catch (error) {
        // Compensate in reverse order
        await this.rollback(data)
        throw error
      }
    }
  }

  private async rollback(data: OrderData): Promise<void> {
    // Execute compensations in reverse order
    for (const step of this.executedSteps.reverse()) {
      try {
        await step.compensate(data)
      } catch (error) {
        // Log but continue - best effort compensation
        logger.error(`Compensation failed for ${step.name}:`, error)
      }
    }
  }
}

// Usage
const saga = new OrderSaga()
try {
  await saga.execute(orderData)
} catch (error) {
  // Saga automatically rolled back
  throw new OrderFailedError('Order could not be completed')
}

// For production: Use orchestrator with persistence
class SagaOrchestrator {
  async execute(sagaId: string, steps: SagaStep[]): Promise<void> {
    // Persist saga state for recovery
    await this.db.sagas.create({
      id: sagaId,
      status: 'running',
      currentStep: 0
    })

    for (let i = 0; i < steps.length; i++) {
      await this.db.sagas.update({
        where: { id: sagaId },
        data: { currentStep: i }
      })

      try {
        await steps[i].execute()
      } catch (error) {
        await this.db.sagas.update({
          where: { id: sagaId },
          data: { status: 'compensating' }
        })
        await this.compensate(sagaId, steps, i)
        throw error
      }
    }

    await this.db.sagas.update({
      where: { id: sagaId },
      data: { status: 'completed' }
    })
  }
}
```

**Why It Works:**
Maintains consistency across services without distributed transactions. Each step is reversible. Progress is trackable. Recovery is possible after crashes.

---

## 7. The Middleware Pattern

**What It Is:**
Processing requests through a chain of handlers, each adding functionality.

**When To Use:**
- When you need cross-cutting concerns (auth, logging, validation)
- When processing is composable
- When you want to separate concerns
- When the same logic applies to many routes

**The Pattern:**

```typescript
// Express-style middleware
type Middleware = (req: Request, res: Response, next: NextFunction) => void

// Authentication middleware
const authenticate: Middleware = async (req, res, next) => {
  const token = req.headers.authorization?.replace('Bearer ', '')

  if (!token) {
    return res.status(401).json({ error: 'No token provided' })
  }

  try {
    const payload = jwt.verify(token, JWT_SECRET)
    req.userId = payload.sub
    next()
  } catch {
    res.status(401).json({ error: 'Invalid token' })
  }
}

// Validation middleware factory
const validate = (schema: ZodSchema): Middleware => (req, res, next) => {
  try {
    req.body = schema.parse(req.body)
    next()
  } catch (error) {
    if (error instanceof ZodError) {
      res.status(400).json({ errors: error.errors })
    } else {
      next(error)
    }
  }
}

// Rate limiting middleware
const rateLimit = (options: RateLimitOptions): Middleware => {
  const limiter = new RateLimiter(options)

  return async (req, res, next) => {
    const key = req.ip
    const allowed = await limiter.check(key)

    if (!allowed) {
      return res.status(429).json({ error: 'Too many requests' })
    }

    next()
  }
}

// Logging middleware
const requestLogger: Middleware = (req, res, next) => {
  const start = Date.now()

  res.on('finish', () => {
    logger.info({
      method: req.method,
      path: req.path,
      status: res.statusCode,
      duration: Date.now() - start
    })
  })

  next()
}

// Error handling middleware (last in chain)
const errorHandler: ErrorMiddleware = (error, req, res, next) => {
  logger.error('Request error:', error)

  if (error instanceof ValidationError) {
    return res.status(400).json({ error: error.message })
  }

  if (error instanceof NotFoundError) {
    return res.status(404).json({ error: error.message })
  }

  res.status(500).json({ error: 'Internal server error' })
}

// Compose middleware
app.use(requestLogger)
app.use(rateLimit({ windowMs: 60000, max: 100 }))

app.post('/api/orders',
  authenticate,
  validate(CreateOrderSchema),
  async (req, res, next) => {
    try {
      const order = await orderService.create(req.body)
      res.status(201).json(order)
    } catch (error) {
      next(error)
    }
  }
)

app.use(errorHandler)
```

**Why It Works:**
Cross-cutting concerns are centralized. Routes stay focused on business logic. Middleware is reusable. Order of execution is explicit.

---

## 8. The Outbox Pattern

**What It Is:**
Ensuring reliable event publishing by storing events in the same transaction as the business operation.

**When To Use:**
- When you need exactly-once event delivery
- When database and message queue must stay in sync
- When events are critical
- When network failures could cause inconsistency

**The Pattern:**

```typescript
// Problem: Database and queue can get out of sync
async function createOrder(data: OrderData): Promise<Order> {
  const order = await db.orders.create({ data })
  await messageQueue.publish('order.created', order) // What if this fails?
  return order
}

// Solution: Outbox pattern
// 1. Store event in same transaction as business operation
async function createOrder(data: OrderData): Promise<Order> {
  return await db.$transaction(async (tx) => {
    const order = await tx.orders.create({ data })

    // Store event in outbox (same transaction)
    await tx.outboxEvents.create({
      data: {
        type: 'order.created',
        payload: JSON.stringify(order),
        status: 'pending'
      }
    })

    return order
  })
  // Either both succeed or both fail
}

// 2. Background worker publishes events from outbox
class OutboxProcessor {
  async process(): Promise<void> {
    // Get pending events
    const events = await db.outboxEvents.findMany({
      where: { status: 'pending' },
      orderBy: { createdAt: 'asc' },
      take: 100
    })

    for (const event of events) {
      try {
        // Publish to queue
        await messageQueue.publish(event.type, JSON.parse(event.payload))

        // Mark as published
        await db.outboxEvents.update({
          where: { id: event.id },
          data: { status: 'published' }
        })
      } catch (error) {
        // Will retry on next run
        logger.error(`Failed to publish event ${event.id}:`, error)
      }
    }
  }
}

// Run processor periodically
setInterval(() => {
  outboxProcessor.process().catch(logger.error)
}, 1000)

// Or use database triggers + polling
// CREATE TABLE outbox_events (
//   id SERIAL PRIMARY KEY,
//   type VARCHAR(255) NOT NULL,
//   payload JSONB NOT NULL,
//   status VARCHAR(50) DEFAULT 'pending',
//   created_at TIMESTAMP DEFAULT NOW()
// );

// Or use Change Data Capture (Debezium)
// Capture INSERT to outbox table â†’ publish to Kafka
```

**Why It Works:**
Event and data change are atomic. No lost events. No duplicate events (with idempotent consumers). Works with any message queue.

---

## 9. The API Versioning Pattern

**What It Is:**
Managing breaking changes to your API while maintaining backward compatibility.

**When To Use:**
- When you have external consumers
- When you need to make breaking changes
- When clients can't update immediately
- When you want controlled deprecation

**The Pattern:**

```typescript
// URL versioning (most common)
app.use('/api/v1', v1Router)
app.use('/api/v2', v2Router)

// v1 endpoints
const v1Router = Router()
v1Router.get('/users/:id', async (req, res) => {
  const user = await userService.getUser(req.params.id)
  res.json({
    id: user.id,
    name: user.fullName, // Old format
    email: user.email
  })
})

// v2 endpoints
const v2Router = Router()
v2Router.get('/users/:id', async (req, res) => {
  const user = await userService.getUser(req.params.id)
  res.json({
    id: user.id,
    firstName: user.firstName, // New format
    lastName: user.lastName,
    email: user.email
  })
})

// Header versioning
const versionMiddleware: Middleware = (req, res, next) => {
  const version = req.headers['api-version'] || 'v1'
  req.apiVersion = version
  next()
}

app.get('/users/:id', versionMiddleware, async (req, res) => {
  const user = await userService.getUser(req.params.id)

  if (req.apiVersion === 'v1') {
    return res.json(transformToV1(user))
  }

  res.json(transformToV2(user))
})

// Response transformers
function transformToV1(user: User): V1User {
  return {
    id: user.id,
    name: `${user.firstName} ${user.lastName}`,
    email: user.email
  }
}

function transformToV2(user: User): V2User {
  return {
    id: user.id,
    firstName: user.firstName,
    lastName: user.lastName,
    email: user.email
  }
}

// Deprecation headers
app.use('/api/v1', (req, res, next) => {
  res.set('Deprecation', 'true')
  res.set('Sunset', 'Sat, 1 Jan 2025 00:00:00 GMT')
  res.set('Link', '</api/v2>; rel="successor-version"')
  next()
})

// Track version usage for migration
const versionMetrics = new Counter({
  name: 'api_requests_by_version',
  help: 'API requests by version',
  labelNames: ['version', 'endpoint']
})

app.use((req, res, next) => {
  versionMetrics.inc({
    version: req.apiVersion,
    endpoint: req.path
  })
  next()
})
```

**Why It Works:**
Clients can migrate at their own pace. Breaking changes don't break existing integrations. Clear deprecation path. Usage metrics inform migration timeline.

---

## 10. The Retry with Backoff Pattern

**What It Is:**
Automatically retrying failed operations with increasing delays between attempts.

**When To Use:**
- When calling external services
- When failures are transient
- When immediate retry won't help
- When you want self-healing behavior

**The Pattern:**

```typescript
interface RetryOptions {
  maxAttempts: number
  initialDelay: number
  maxDelay: number
  factor: number // Exponential factor
  retryOn?: (error: Error) => boolean
}

async function withRetry<T>(
  fn: () => Promise<T>,
  options: RetryOptions
): Promise<T> {
  let lastError: Error
  let delay = options.initialDelay

  for (let attempt = 1; attempt <= options.maxAttempts; attempt++) {
    try {
      return await fn()
    } catch (error) {
      lastError = error as Error

      // Check if error is retryable
      if (options.retryOn && !options.retryOn(lastError)) {
        throw lastError
      }

      if (attempt === options.maxAttempts) {
        throw lastError
      }

      // Wait with exponential backoff + jitter
      const jitter = Math.random() * 0.3 * delay // 0-30% jitter
      await sleep(delay + jitter)

      // Increase delay for next attempt
      delay = Math.min(delay * options.factor, options.maxDelay)

      logger.warn(`Attempt ${attempt} failed, retrying in ${delay}ms`, {
        error: lastError.message
      })
    }
  }

  throw lastError!
}

// Usage
const result = await withRetry(
  () => externalApiCall(),
  {
    maxAttempts: 3,
    initialDelay: 1000,
    maxDelay: 10000,
    factor: 2,
    retryOn: (error) => {
      // Only retry on transient errors
      if (error instanceof HttpError) {
        return [408, 429, 500, 502, 503, 504].includes(error.status)
      }
      return false
    }
  }
)

// With libraries (p-retry)
import pRetry from 'p-retry'

const result = await pRetry(
  async () => {
    const response = await fetch('https://api.example.com/data')
    if (!response.ok) {
      throw new pRetry.AbortError('Not retryable') // Or throw normally to retry
    }
    return response.json()
  },
  {
    retries: 3,
    onFailedAttempt: (error) => {
      logger.warn(`Attempt ${error.attemptNumber} failed. ${error.retriesLeft} retries left.`)
    }
  }
)

// Retry patterns:
// - Exponential backoff: 1s, 2s, 4s, 8s...
// - Linear backoff: 1s, 2s, 3s, 4s...
// - Constant: 1s, 1s, 1s, 1s...
// - Fibonacci: 1s, 1s, 2s, 3s, 5s...

// Always add jitter to prevent thundering herd
function addJitter(delay: number, factor = 0.3): number {
  return delay + (Math.random() * factor * delay)
}
```

**Why It Works:**
Transient failures are handled automatically. Backoff prevents overwhelming failing services. Jitter prevents synchronized retries. Gives services time to recover.

## Anti-Patterns

# Anti-Patterns: Backend Engineering

These approaches look like reasonable backend code but consistently lead to outages, data corruption, and unmaintainable systems.

---

## 1. The God Service

**The Mistake:**
```typescript
class ApplicationService {
  async createUser(data: UserData) { /* ... */ }
  async updateUser(id: string, data: UserData) { /* ... */ }
  async deleteUser(id: string) { /* ... */ }
  async createOrder(data: OrderData) { /* ... */ }
  async processPayment(orderId: string) { /* ... */ }
  async shipOrder(orderId: string) { /* ... */ }
  async sendEmail(to: string, template: string) { /* ... */ }
  async generateReport(type: string) { /* ... */ }
  async uploadFile(file: Buffer) { /* ... */ }
  async processWebhook(data: any) { /* ... */ }
  // ... 50 more methods covering everything
}

// Used everywhere
const app = new ApplicationService()
app.createUser(...)
app.processPayment(...)
app.sendEmail(...)
```

**Why It's Wrong:**
- Impossible to understand at a glance
- Can't test individual behaviors
- Changes to one domain affect everything
- No clear ownership
- Grows forever

**The Fix:**
```typescript
// Separate services by domain
class UserService {
  async create(data: UserData): Promise<User> { /* ... */ }
  async update(id: string, data: UserData): Promise<User> { /* ... */ }
  async delete(id: string): Promise<void> { /* ... */ }
}

class OrderService {
  async create(data: OrderData): Promise<Order> { /* ... */ }
  async process(id: string): Promise<Order> { /* ... */ }
  async ship(id: string): Promise<Order> { /* ... */ }
}

class PaymentService {
  async charge(amount: number): Promise<Payment> { /* ... */ }
  async refund(paymentId: string): Promise<Refund> { /* ... */ }
}

// Each service has single responsibility
// Each can be tested independently
// Each can evolve independently
```

---

## 2. The Leaky Abstraction

**The Mistake:**
```typescript
// Repository that exposes database internals
class UserRepository {
  async find(query: any): Promise<User[]> {
    // Caller needs to know MongoDB query syntax
    return this.collection.find(query).toArray()
  }

  async update(filter: any, update: any): Promise<void> {
    // Caller needs to know $set, $inc, etc.
    await this.collection.updateOne(filter, update)
  }
}

// Usage - database knowledge leaked everywhere
await userRepo.find({ 'profile.age': { $gte: 18 } })
await userRepo.update(
  { _id: userId },
  { $set: { name: newName }, $inc: { loginCount: 1 } }
)
```

**Why It's Wrong:**
- Can't change database without changing all callers
- Business logic mixed with query syntax
- Impossible to add caching transparently
- Testing requires database knowledge

**The Fix:**
```typescript
// Repository with business-oriented interface
class UserRepository {
  async findByAge(minAge: number): Promise<User[]> {
    return this.collection.find({ 'profile.age': { $gte: minAge } }).toArray()
  }

  async updateName(userId: string, name: string): Promise<void> {
    await this.collection.updateOne(
      { _id: userId },
      { $set: { name } }
    )
  }

  async recordLogin(userId: string): Promise<void> {
    await this.collection.updateOne(
      { _id: userId },
      { $inc: { loginCount: 1 }, $set: { lastLogin: new Date() } }
    )
  }
}

// Usage - no database knowledge needed
await userRepo.findByAge(18)
await userRepo.updateName(userId, newName)
await userRepo.recordLogin(userId)

// Can now change to Postgres without changing callers
```

---

## 3. The Scattered Validation

**The Mistake:**
```typescript
// Validation scattered across the codebase
async function createOrder(req, res) {
  // Validation in controller
  if (!req.body.items || req.body.items.length === 0) {
    return res.status(400).json({ error: 'Items required' })
  }

  for (const item of req.body.items) {
    // More validation
    if (!item.productId || !item.quantity) {
      return res.status(400).json({ error: 'Invalid item' })
    }
    if (item.quantity < 1) {
      return res.status(400).json({ error: 'Quantity must be positive' })
    }
  }

  // Different validation in service
  const service = new OrderService()
  // Service does its own validation...
}

// Model has its own validation
class Order {
  setItems(items: OrderItem[]) {
    // Yet another validation
    if (items.length > 100) {
      throw new Error('Too many items')
    }
  }
}
```

**Why It's Wrong:**
- Validation logic duplicated
- Easy to miss validation in some paths
- Inconsistent error messages
- Hard to know all the rules
- Testing requires checking multiple layers

**The Fix:**
```typescript
// Single source of truth for validation
import { z } from 'zod'

const OrderItemSchema = z.object({
  productId: z.string().uuid(),
  quantity: z.number().int().min(1).max(100)
})

const CreateOrderSchema = z.object({
  items: z.array(OrderItemSchema).min(1).max(100)
})

// Validation middleware
const validate = (schema: ZodSchema) => (req, res, next) => {
  try {
    req.body = schema.parse(req.body)
    next()
  } catch (error) {
    if (error instanceof ZodError) {
      return res.status(400).json({
        error: 'Validation failed',
        details: error.errors
      })
    }
    next(error)
  }
}

// Controller just uses validated data
app.post('/orders', validate(CreateOrderSchema), async (req, res) => {
  // req.body is validated and typed
  const order = await orderService.create(req.body)
  res.json(order)
})

// Service trusts input is validated
class OrderService {
  async create(data: z.infer<typeof CreateOrderSchema>) {
    // Data is already validated
    return this.orders.create(data)
  }
}
```

---

## 4. The HTTP-in-Service

**The Mistake:**
```typescript
// Service knows about HTTP
class UserService {
  async createUser(req: Request, res: Response) {
    try {
      const user = await this.db.users.create({ data: req.body })
      res.status(201).json(user)
    } catch (error) {
      if (error.code === 'P2002') {
        res.status(409).json({ error: 'Email already exists' })
      } else {
        res.status(500).json({ error: 'Server error' })
      }
    }
  }

  async getUser(req: Request, res: Response) {
    const user = await this.db.users.findUnique({
      where: { id: req.params.id }
    })

    if (!user) {
      return res.status(404).json({ error: 'User not found' })
    }

    res.json(user)
  }
}
```

**Why It's Wrong:**
- Can't use service from CLI, tests, or other services
- HTTP status codes mixed with business logic
- Service is coupled to Express
- Testing requires mocking Request/Response
- Can't reuse logic

**The Fix:**
```typescript
// Domain errors (no HTTP knowledge)
class EmailExistsError extends Error {
  constructor(email: string) {
    super(`Email ${email} already exists`)
  }
}

class UserNotFoundError extends Error {
  constructor(id: string) {
    super(`User ${id} not found`)
  }
}

// Service is pure business logic
class UserService {
  async createUser(data: CreateUserData): Promise<User> {
    try {
      return await this.db.users.create({ data })
    } catch (error) {
      if (error.code === 'P2002') {
        throw new EmailExistsError(data.email)
      }
      throw error
    }
  }

  async getUser(id: string): Promise<User> {
    const user = await this.db.users.findUnique({ where: { id } })
    if (!user) {
      throw new UserNotFoundError(id)
    }
    return user
  }
}

// Controller handles HTTP translation
app.post('/users', async (req, res, next) => {
  try {
    const user = await userService.createUser(req.body)
    res.status(201).json(user)
  } catch (error) {
    next(error) // To error middleware
  }
})

// Error middleware translates to HTTP
app.use((error, req, res, next) => {
  if (error instanceof EmailExistsError) {
    return res.status(409).json({ error: error.message })
  }
  if (error instanceof UserNotFoundError) {
    return res.status(404).json({ error: error.message })
  }
  res.status(500).json({ error: 'Server error' })
})
```

---

## 5. The Synchronous Everything

**The Mistake:**
```typescript
// Everything happens in the request
app.post('/api/orders', async (req, res) => {
  // Create order
  const order = await orderService.create(req.body)

  // Process payment (external API, slow)
  const payment = await paymentService.charge(order.total)

  // Send confirmation email (external API, slow)
  await emailService.sendConfirmation(order)

  // Update inventory (database operations)
  await inventoryService.reserve(order.items)

  // Create shipping label (external API, slow)
  const label = await shippingService.createLabel(order)

  // Update analytics (external API)
  await analyticsService.trackOrder(order)

  // Generate invoice PDF (CPU intensive)
  const invoice = await pdfService.generateInvoice(order)

  res.json({ order, label, invoice })
  // Total time: 15+ seconds, user staring at spinner
})
```

**Why It's Wrong:**
- Request takes forever
- Any failure fails the whole request
- User has to wait for non-critical operations
- Can't retry individual steps
- Timeout issues

**The Fix:**
```typescript
// Only critical operations in request
app.post('/api/orders', async (req, res) => {
  // Create order with pending status
  const order = await orderService.create({
    ...req.body,
    status: 'pending'
  })

  // Queue background jobs
  await queue.add('order:process', { orderId: order.id })

  // Return immediately
  res.status(202).json({
    order,
    message: 'Order received, processing'
  })
})

// Background worker handles the rest
const worker = new Worker('order:process', async (job) => {
  const { orderId } = job.data

  // Steps can be individual jobs
  await queue.add('payment:charge', { orderId })
  await queue.add('email:confirmation', { orderId })
  await queue.add('inventory:reserve', { orderId })
  await queue.add('shipping:label', { orderId })
  await queue.add('analytics:track', { orderId })
  await queue.add('pdf:invoice', { orderId })
})

// Each job is independent, retryable
const paymentWorker = new Worker('payment:charge', async (job) => {
  const { orderId } = job.data
  const order = await orderService.get(orderId)
  const payment = await paymentService.charge(order.total)
  await orderService.update(orderId, { paymentId: payment.id })
}, {
  attempts: 3,
  backoff: { type: 'exponential', delay: 1000 }
})
```

---

## 6. The String SQL

**The Mistake:**
```typescript
// String concatenation SQL
async function findUsers(filters: UserFilters) {
  let sql = 'SELECT * FROM users WHERE 1=1'

  if (filters.name) {
    sql += ` AND name LIKE '%${filters.name}%'`
  }
  if (filters.email) {
    sql += ` AND email = '${filters.email}'`
  }
  if (filters.role) {
    sql += ` AND role = '${filters.role}'`
  }
  if (filters.sortBy) {
    sql += ` ORDER BY ${filters.sortBy}`
  }
  if (filters.limit) {
    sql += ` LIMIT ${filters.limit}`
  }

  return db.query(sql)
}

// Attacker: filters.email = "'; DROP TABLE users; --"
// Result: All users deleted
```

**Why It's Wrong:**
- SQL injection vulnerability
- Can lead to data breach or deletion
- Hard to maintain and debug
- Type information lost

**The Fix:**
```typescript
// Parameterized queries
async function findUsers(filters: UserFilters) {
  const conditions: string[] = []
  const params: any[] = []
  let paramCount = 0

  if (filters.name) {
    paramCount++
    conditions.push(`name ILIKE $${paramCount}`)
    params.push(`%${filters.name}%`)
  }
  if (filters.email) {
    paramCount++
    conditions.push(`email = $${paramCount}`)
    params.push(filters.email)
  }
  if (filters.role) {
    paramCount++
    conditions.push(`role = $${paramCount}`)
    params.push(filters.role)
  }

  // Whitelist for ORDER BY (not parameterizable)
  const allowedSorts = ['name', 'email', 'created_at']
  const sortBy = allowedSorts.includes(filters.sortBy)
    ? filters.sortBy
    : 'created_at'

  const where = conditions.length > 0
    ? `WHERE ${conditions.join(' AND ')}`
    : ''

  const sql = `
    SELECT * FROM users
    ${where}
    ORDER BY ${sortBy}
    LIMIT $${paramCount + 1}
  `

  return db.query(sql, [...params, filters.limit || 100])
}

// Better: Use query builder or ORM
async function findUsers(filters: UserFilters) {
  return prisma.user.findMany({
    where: {
      name: filters.name ? { contains: filters.name } : undefined,
      email: filters.email,
      role: filters.role
    },
    orderBy: { [filters.sortBy || 'createdAt']: 'asc' },
    take: filters.limit || 100
  })
}
```

---

## 7. The Missing Transaction

**The Mistake:**
```typescript
// Multiple related operations without transaction
async function transferMoney(fromId: string, toId: string, amount: number) {
  // What if the second update fails?
  await db.accounts.update({
    where: { id: fromId },
    data: { balance: { decrement: amount } }
  })

  // Crash here = money disappeared!

  await db.accounts.update({
    where: { id: toId },
    data: { balance: { increment: amount } }
  })
}

// Creating related records
async function createOrderWithItems(orderData: OrderData, items: ItemData[]) {
  const order = await db.orders.create({ data: orderData })

  // If this fails, orphaned order exists
  for (const item of items) {
    await db.orderItems.create({
      data: { ...item, orderId: order.id }
    })
  }

  return order
}
```

**Why It's Wrong:**
- Partial failures leave inconsistent state
- Data integrity violations
- Money can disappear
- Orphaned records
- Race conditions

**The Fix:**
```typescript
// Use transactions for related operations
async function transferMoney(fromId: string, toId: string, amount: number) {
  await prisma.$transaction(async (tx) => {
    // Check balance
    const from = await tx.accounts.findUnique({ where: { id: fromId } })
    if (from.balance < amount) {
      throw new InsufficientBalanceError()
    }

    await tx.accounts.update({
      where: { id: fromId },
      data: { balance: { decrement: amount } }
    })

    await tx.accounts.update({
      where: { id: toId },
      data: { balance: { increment: amount } }
    })
  })
  // Either both succeed or both fail
}

// Create related records atomically
async function createOrderWithItems(orderData: OrderData, items: ItemData[]) {
  return prisma.order.create({
    data: {
      ...orderData,
      items: {
        create: items // Nested create in same transaction
      }
    },
    include: { items: true }
  })
}

// Or explicit transaction
async function createOrderWithItems(orderData: OrderData, items: ItemData[]) {
  return prisma.$transaction(async (tx) => {
    const order = await tx.orders.create({ data: orderData })

    await tx.orderItems.createMany({
      data: items.map(item => ({ ...item, orderId: order.id }))
    })

    return order
  })
}
```

---

## 8. The Massive Migration

**The Mistake:**
```sql
-- One migration that modifies millions of rows
ALTER TABLE users ADD COLUMN preferences JSONB;

UPDATE users
SET preferences = json_build_object(
  'theme', 'light',
  'notifications', true,
  'language', 'en'
);

ALTER TABLE users ALTER COLUMN preferences SET NOT NULL;

-- Locks table for 30 minutes, site is down
```

**Why It's Wrong:**
- Long-running table lock
- Downtime for users
- Can't be easily rolled back
- Overwhelms database resources
- Blocks all writes to table

**The Fix:**
```sql
-- Migration 1: Add nullable column (instant)
ALTER TABLE users ADD COLUMN preferences JSONB;

-- Migration 2: Backfill in batches (background job)
-- Run during low traffic, can pause/resume
DO $$
DECLARE
  batch_size INT := 10000;
  last_id INT := 0;
  rows_updated INT;
BEGIN
  LOOP
    UPDATE users
    SET preferences = json_build_object(
      'theme', 'light',
      'notifications', true,
      'language', 'en'
    )
    WHERE id > last_id
      AND id <= last_id + batch_size
      AND preferences IS NULL;

    GET DIAGNOSTICS rows_updated = ROW_COUNT;
    EXIT WHEN rows_updated = 0;

    last_id := last_id + batch_size;

    -- Brief pause to let other operations through
    PERFORM pg_sleep(0.1);
  END LOOP;
END $$;

-- Migration 3: Add NOT NULL (after backfill complete)
ALTER TABLE users ALTER COLUMN preferences SET NOT NULL;

-- Or use application default during transition
class User {
  get preferences() {
    return this._preferences ?? DEFAULT_PREFERENCES
  }
}
```

---

## 9. The Chatty Service

**The Mistake:**
```typescript
// Many small requests to get related data
async function getOrderDetails(orderId: string) {
  const order = await db.orders.findUnique({ where: { id: orderId } })

  // N+1 on items
  const items = []
  for (const itemId of order.itemIds) {
    const item = await db.items.findUnique({ where: { id: itemId } })
    items.push(item)
  }

  // Separate call for user
  const user = await db.users.findUnique({ where: { id: order.userId } })

  // Separate call for shipping
  const shipping = await db.shipping.findUnique({
    where: { orderId: order.id }
  })

  // Separate call for payment
  const payment = await db.payments.findUnique({
    where: { orderId: order.id }
  })

  return { order, items, user, shipping, payment }
}
// 5+ database round trips minimum
```

**Why It's Wrong:**
- Many round trips to database
- High latency
- More network overhead
- Database connection pool exhaustion
- Scales poorly

**The Fix:**
```typescript
// Single query with includes
async function getOrderDetails(orderId: string) {
  return db.orders.findUnique({
    where: { id: orderId },
    include: {
      items: true,
      user: true,
      shipping: true,
      payment: true
    }
  })
}
// 1 round trip (or 2 with some ORMs)

// For complex cases, use DataLoader
import DataLoader from 'dataloader'

const userLoader = new DataLoader(async (userIds: string[]) => {
  const users = await db.users.findMany({
    where: { id: { in: userIds } }
  })
  // Return in same order as input
  return userIds.map(id => users.find(u => u.id === id))
})

// Usage - batches and caches automatically
const user1 = await userLoader.load('user-1')
const user2 = await userLoader.load('user-2')
// Only one query: SELECT * FROM users WHERE id IN ('user-1', 'user-2')

// For cross-service: GraphQL federation or API composition
async function getOrderDetails(orderId: string) {
  const [order, shipping, payment] = await Promise.all([
    orderService.get(orderId),
    shippingService.getByOrderId(orderId),
    paymentService.getByOrderId(orderId)
  ])
  // Parallel calls instead of sequential
  return { order, shipping, payment }
}
```

---

## 10. The Implicit Coupling

**The Mistake:**
```typescript
// Services coupled through shared database
class OrderService {
  async createOrder(data: OrderData) {
    // Directly updates inventory table
    await db.inventory.update({
      where: { productId: data.productId },
      data: { quantity: { decrement: data.quantity } }
    })

    return db.orders.create({ data })
  }
}

class InventoryService {
  async checkStock(productId: string) {
    // Doesn't know orders modifies this
    return db.inventory.findUnique({ where: { productId } })
  }

  async restockProduct(productId: string, quantity: number) {
    // Order service also touches this table
    await db.inventory.update({
      where: { productId },
      data: { quantity: { increment: quantity } }
    })
  }
}
```

**Why It's Wrong:**
- Hidden dependencies
- Inventory logic in two places
- Can't change inventory table without checking all services
- Race conditions possible
- Testing requires full database

**The Fix:**
```typescript
// Explicit dependencies through interfaces
class OrderService {
  constructor(
    private orders: OrderRepository,
    private inventory: InventoryService // Explicit dependency
  ) {}

  async createOrder(data: OrderData) {
    // Use inventory service, don't touch its table
    const available = await this.inventory.checkAndReserve(
      data.productId,
      data.quantity
    )

    if (!available) {
      throw new InsufficientStockError()
    }

    return this.orders.create(data)
  }
}

class InventoryService {
  constructor(private inventory: InventoryRepository) {}

  async checkStock(productId: string): Promise<number> {
    const item = await this.inventory.findById(productId)
    return item?.quantity ?? 0
  }

  async checkAndReserve(productId: string, quantity: number): Promise<boolean> {
    // Single owner of inventory logic
    const result = await this.inventory.decrementIfSufficient(
      productId,
      quantity
    )
    return result.success
  }

  async restockProduct(productId: string, quantity: number): Promise<void> {
    await this.inventory.increment(productId, quantity)
  }
}
```

---

## 11. The Logging Afterthought

**The Mistake:**
```typescript
// Minimal or missing logging
async function processPayment(orderId: string) {
  try {
    const order = await db.orders.findUnique({ where: { id: orderId } })
    const payment = await stripe.charges.create({ amount: order.total })
    await db.orders.update({
      where: { id: orderId },
      data: { paymentId: payment.id, status: 'paid' }
    })
    return payment
  } catch (error) {
    console.log('Payment failed') // Useless
    throw error
  }
}

// Or worse, logging sensitive data
console.log('Processing payment for user:', user)
// Logs entire user object including password hash
```

**Why It's Wrong:**
- Can't debug production issues
- No context when things fail
- Sensitive data in logs
- Can't trace request flow
- No performance metrics

**The Fix:**
```typescript
import { logger } from './logger' // Structured logger

async function processPayment(orderId: string) {
  const logContext = { orderId, operation: 'processPayment' }

  logger.info('Starting payment processing', logContext)

  try {
    const order = await db.orders.findUnique({ where: { id: orderId } })

    if (!order) {
      logger.warn('Order not found', logContext)
      throw new OrderNotFoundError(orderId)
    }

    logger.info('Charging payment', {
      ...logContext,
      amount: order.total,
      currency: 'usd'
    })

    const payment = await stripe.charges.create({ amount: order.total })

    logger.info('Payment successful', {
      ...logContext,
      paymentId: payment.id
    })

    await db.orders.update({
      where: { id: orderId },
      data: { paymentId: payment.id, status: 'paid' }
    })

    return payment
  } catch (error) {
    logger.error('Payment processing failed', {
      ...logContext,
      error: error.message,
      stack: error.stack,
      // Never log full error if it might contain sensitive data
    })
    throw error
  }
}

// Structured logging with correlation
const requestLogger = (req, res, next) => {
  const requestId = crypto.randomUUID()
  req.logger = logger.child({ requestId })

  req.logger.info('Request started', {
    method: req.method,
    path: req.path,
    // Don't log body (might contain sensitive data)
  })

  next()
}
```

---

## 12. The Premature Microservices

**The Mistake:**
```
# "Let's start with microservices for scalability"

services/
â”œâ”€â”€ user-service/
â”œâ”€â”€ order-service/
â”œâ”€â”€ payment-service/
â”œâ”€â”€ notification-service/
â”œâ”€â”€ inventory-service/
â”œâ”€â”€ shipping-service/
â”œâ”€â”€ analytics-service/
â”œâ”€â”€ auth-service/
â””â”€â”€ api-gateway/

# For a team of 3 people, with 100 users
```

**Why It's Wrong:**
- Operational overhead before you need it
- Network latency between every call
- Distributed debugging nightmare
- Team is too small to own all services
- Premature boundaries often wrong

**The Fix:**
```
# Start monolithic, extract when needed

src/
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ users/
â”‚   â”‚   â”œâ”€â”€ user.service.ts
â”‚   â”‚   â”œâ”€â”€ user.controller.ts
â”‚   â”‚   â””â”€â”€ user.repository.ts
â”‚   â”œâ”€â”€ orders/
â”‚   â”œâ”€â”€ payments/
â”‚   â””â”€â”€ notifications/
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ auth/
â”‚   â””â”€â”€ utils/
â””â”€â”€ app.ts

# Extract to microservice when:
# - Module has different scaling needs
# - Module has different team ownership
# - Module has different deployment frequency
# - Module is genuinely independent
# - You have operational maturity (CI/CD, monitoring, logging)

# Usually: When you have 50+ engineers, not before
```

## Sharp Edges (Gotchas)

*Real production issues that cause outages and bugs.*

# Sharp Edges: Backend Engineering

These are the backend mistakes that cause outages, data corruption, and 3am pages. Each edge represents real incidents, lost data, and careers changed by "it worked in dev."

---

## 1. The N+1 Query Massacre

**Severity:** Critical

**The Trap:**
Your API endpoint returns users with their posts. ORM makes it easy - just access `user.posts` in a loop. Works great with 10 users. With 1,000 users, you've just fired 1,001 queries. Database CPU spikes, response times balloon, users complain about slowness.

**Why It Happens:**
ORMs hide the queries. Lazy loading feels convenient. Testing with small datasets masks the problem. No query monitoring in development.

**The Fix:**
```typescript
// WRONG - N+1 (fires query for each user's posts)
const users = await prisma.user.findMany()
for (const user of users) {
  const posts = await prisma.post.findMany({
    where: { authorId: user.id }
  })
  // 1 + N queries
}

// WRONG - Still N+1 (ORM lazy loads)
const users = await prisma.user.findMany()
users.map(u => u.posts) // Each access fires a query

// RIGHT - Eager loading with include
const users = await prisma.user.findMany({
  include: { posts: true }
})
// 1 query with JOIN, or 2 queries with IN clause

// RIGHT - Explicit join in SQL
SELECT u.*, p.*
FROM users u
LEFT JOIN posts p ON p.author_id = u.id

// Detection: Enable query logging
// Look for repeated similar queries
// Monitor query count per request (should be low)
```

**Detection Pattern:**
- Response time grows linearly with data size
- Database CPU high, app CPU low
- Query logs show repeated similar queries
- Pagination doesn't improve performance

---

## 2. The Transaction Timeout Trap

**Severity:** Critical

**The Trap:**
You wrap a complex operation in a transaction for safety. Part of that operation calls an external API. The API is slow today. Your transaction holds locks for 30 seconds. Other requests queue up waiting for locks. Database connections exhaust. Everything freezes.

**Why It Happens:**
"Transactions make it safe" oversimplification. External calls in transactions feel logical. No timeout configuration. Testing doesn't simulate slow externals.

**The Fix:**
```typescript
// WRONG - External call inside transaction
await prisma.$transaction(async (tx) => {
  const order = await tx.order.create({ data: orderData })

  // This can take 30+ seconds if payment service is slow
  const payment = await paymentService.charge(order.total)

  await tx.order.update({
    where: { id: order.id },
    data: { paymentId: payment.id }
  })
})

// RIGHT - External calls outside transaction
// 1. Create order in pending state
const order = await prisma.order.create({
  data: { ...orderData, status: 'pending_payment' }
})

// 2. External call without transaction
const payment = await paymentService.charge(order.total)

// 3. Quick transaction to update
await prisma.order.update({
  where: { id: order.id },
  data: { paymentId: payment.id, status: 'paid' }
})

// BETTER - Saga pattern for complex flows
// Each step is independent with compensation on failure

// Always set transaction timeout
await prisma.$transaction(
  async (tx) => { /* ... */ },
  { timeout: 5000 } // 5 second max
)
```

**Detection Pattern:**
- Database lock wait timeouts
- Connection pool exhaustion
- "Waiting for lock" in slow query log
- External service latency correlates with DB issues

---

## 3. The Uncached Repeated Query

**Severity:** High

**The Trap:**
Every page load queries user preferences. Every API call validates permissions. You're hitting the database thousands of times for data that changes once a day. Database groans under the load of serving the same data repeatedly.

**Why It Happens:**
"Database is fast" assumption. Fear of stale data. No caching strategy. Each developer adds queries without seeing the full picture.

**The Fix:**
```typescript
// WRONG - Query on every request
async function getUser(req, res) {
  const user = await db.user.findUnique({
    where: { id: req.userId },
    include: { preferences: true, permissions: true }
  })
  // Called 10,000 times/minute for same user
}

// RIGHT - Cache with appropriate TTL
import { Redis } from 'ioredis'
const redis = new Redis()

async function getUser(userId: string) {
  const cacheKey = `user:${userId}`

  // Try cache first
  const cached = await redis.get(cacheKey)
  if (cached) return JSON.parse(cached)

  // Cache miss - query DB
  const user = await db.user.findUnique({
    where: { id: userId },
    include: { preferences: true }
  })

  // Cache for 5 minutes
  await redis.setex(cacheKey, 300, JSON.stringify(user))

  return user
}

// Invalidate on update
async function updateUser(userId: string, data: UserData) {
  await db.user.update({ where: { id: userId }, data })
  await redis.del(`user:${userId}`) // Invalidate cache
}

// Cache patterns by data type:
// - User session: 15-60 minutes
// - User preferences: 5-15 minutes
// - Feature flags: 1-5 minutes
// - Rate limits: Seconds
// - Static config: Hours
```

**Detection Pattern:**
- Same query appears thousands of times in logs
- Database load doesn't match business logic complexity
- Adding Redis dramatically improves performance
- Query is simple but takes significant total time

---

## 4. The Missing Idempotency Key

**Severity:** Critical

**The Trap:**
User clicks "Pay" twice. Network hiccup causes retry. Webhook fires twice. Without idempotency, you've just charged them twice, created duplicate orders, or sent the email twice. Now you're dealing with angry customers and refunds.

**Why It Happens:**
Happy path thinking. "Users won't do that." Network is assumed reliable. Retry logic without deduplication. Webhooks don't guarantee once-delivery.

**The Fix:**
```typescript
// WRONG - No idempotency
app.post('/api/charge', async (req, res) => {
  const { userId, amount } = req.body
  await paymentService.charge(userId, amount) // Can double-charge
  await db.order.create({ data: { userId, amount } }) // Duplicate order
})

// RIGHT - Idempotency key
app.post('/api/charge', async (req, res) => {
  const { userId, amount, idempotencyKey } = req.body

  // Check if we've seen this request before
  const existing = await db.payment.findUnique({
    where: { idempotencyKey }
  })

  if (existing) {
    // Return cached response, don't process again
    return res.json(existing.response)
  }

  // Process payment
  const result = await paymentService.charge(userId, amount)

  // Store idempotency record
  await db.payment.create({
    data: {
      idempotencyKey,
      userId,
      amount,
      response: result
    }
  })

  res.json(result)
})

// For webhooks - use event ID as idempotency key
app.post('/webhook/stripe', async (req, res) => {
  const { id: eventId, type, data } = req.body

  const processed = await db.webhookEvent.findUnique({
    where: { eventId }
  })

  if (processed) {
    return res.status(200).send('Already processed')
  }

  await processWebhook(type, data)
  await db.webhookEvent.create({ data: { eventId } })

  res.status(200).send('OK')
})
```

**Detection Pattern:**
- Customer complaints about double charges
- Duplicate records in database
- Audit logs show same operation twice close together
- Payment disputes for duplicates

---

## 5. The Unvalidated Input Injection

**Severity:** Critical

**The Trap:**
User input goes into a query without sanitization. Attacker crafts special input. Your database executes their commands. They've just downloaded your entire user table, or worse, dropped it.

**Why It Happens:**
String concatenation is easy. ORMs give false security sense. Input "looks fine" in testing. Validation is tedious.

**The Fix:**
```typescript
// WRONG - SQL Injection vulnerable
const query = `SELECT * FROM users WHERE email = '${email}'`
// Attacker sends: email = "'; DROP TABLE users; --"

// WRONG - NoSQL Injection
const user = await db.users.findOne({
  email: req.body.email // If email is {$gt: ""}, returns all users
})

// RIGHT - Parameterized queries
const query = 'SELECT * FROM users WHERE email = $1'
const result = await db.query(query, [email])

// RIGHT - ORM with proper typing
const user = await prisma.user.findUnique({
  where: { email: String(req.body.email) } // Explicit type coercion
})

// RIGHT - Input validation with Zod
import { z } from 'zod'

const UserInput = z.object({
  email: z.string().email(),
  name: z.string().min(1).max(100),
  age: z.number().int().positive().optional()
})

app.post('/users', async (req, res) => {
  const input = UserInput.parse(req.body) // Throws if invalid
  const user = await prisma.user.create({ data: input })
  res.json(user)
})

// Validate EVERYTHING from user:
// - Query params
// - Body
// - Headers (some attacks via headers)
// - File uploads
```

**Detection Pattern:**
- String concatenation with user input in queries
- No input validation middleware
- Errors contain SQL syntax
- WAF logs show injection attempts

---

## 6. The Unbounded Query

**Severity:** High

**The Trap:**
Admin requests "all users." There are 10 million users. Query runs for 60 seconds, uses 8GB RAM, crashes the server. Or worse, it succeeds and returns a 2GB JSON response that crashes the client.

**Why It Happens:**
"We don't have that much data" assumption. No pagination by default. Endpoints work in dev with 100 records. No resource limits.

**The Fix:**
```typescript
// WRONG - No limits
app.get('/api/users', async (req, res) => {
  const users = await db.user.findMany() // All 10 million
  res.json(users)
})

// RIGHT - Always paginate
app.get('/api/users', async (req, res) => {
  const page = parseInt(req.query.page) || 1
  const limit = Math.min(parseInt(req.query.limit) || 20, 100) // Max 100
  const skip = (page - 1) * limit

  const [users, total] = await Promise.all([
    db.user.findMany({ skip, take: limit }),
    db.user.count()
  ])

  res.json({
    data: users,
    pagination: {
      page,
      limit,
      total,
      pages: Math.ceil(total / limit)
    }
  })
})

// RIGHT - Cursor pagination for large datasets
app.get('/api/users', async (req, res) => {
  const cursor = req.query.cursor
  const limit = Math.min(parseInt(req.query.limit) || 20, 100)

  const users = await db.user.findMany({
    take: limit + 1, // Fetch one extra to check if more exist
    cursor: cursor ? { id: cursor } : undefined,
    orderBy: { id: 'asc' }
  })

  const hasMore = users.length > limit
  const data = hasMore ? users.slice(0, -1) : users

  res.json({
    data,
    nextCursor: hasMore ? data[data.length - 1].id : null
  })
})

// Also protect internal queries
async function processAllUsers() {
  let cursor = undefined
  while (true) {
    const batch = await db.user.findMany({
      take: 1000,
      cursor,
      orderBy: { id: 'asc' }
    })
    if (batch.length === 0) break

    await processBatch(batch)
    cursor = { id: batch[batch.length - 1].id }
  }
}
```

**Detection Pattern:**
- Endpoints that can return unlimited data
- Memory spikes during certain requests
- Client timeouts on list endpoints
- Database slow queries on SELECT without LIMIT

---

## 7. The Race Condition Balance

**Severity:** Critical

**The Trap:**
User has $100 balance. Two concurrent requests each check balance, see $100, each deduct $80. Both succeed. Balance is now -$60. You've given away free money.

**Why It Happens:**
Check-then-act patterns. Optimistic concurrency feels fast. No transaction isolation understanding. Hard to reproduce in testing.

**The Fix:**
```typescript
// WRONG - Race condition
async function withdraw(userId: string, amount: number) {
  const user = await db.user.findUnique({ where: { id: userId } })

  if (user.balance >= amount) { // Check
    await db.user.update({
      where: { id: userId },
      data: { balance: user.balance - amount } // Act with stale value
    })
  }
}

// RIGHT - Atomic update with condition
async function withdraw(userId: string, amount: number) {
  const result = await db.user.updateMany({
    where: {
      id: userId,
      balance: { gte: amount } // Condition in WHERE
    },
    data: {
      balance: { decrement: amount } // Atomic decrement
    }
  })

  if (result.count === 0) {
    throw new Error('Insufficient balance')
  }
}

// RIGHT - Pessimistic locking
async function withdraw(userId: string, amount: number) {
  await prisma.$transaction(async (tx) => {
    // SELECT ... FOR UPDATE
    const user = await tx.$queryRaw`
      SELECT * FROM users WHERE id = ${userId} FOR UPDATE
    `

    if (user.balance < amount) {
      throw new Error('Insufficient balance')
    }

    await tx.user.update({
      where: { id: userId },
      data: { balance: { decrement: amount } }
    })
  })
}

// RIGHT - Optimistic locking with version
async function withdraw(userId: string, amount: number, expectedVersion: number) {
  const result = await db.user.updateMany({
    where: {
      id: userId,
      version: expectedVersion // Only update if version matches
    },
    data: {
      balance: { decrement: amount },
      version: { increment: 1 }
    }
  })

  if (result.count === 0) {
    throw new ConcurrencyError('Please retry')
  }
}
```

**Detection Pattern:**
- Financial discrepancies in audits
- "Impossible" states in data
- Check-then-act patterns in code
- No locking or atomic operations on shared resources

---

## 8. The Cascading Delete Disaster

**Severity:** Critical

**The Trap:**
User deletes their account. CASCADE DELETE triggers. Their posts delete. Comments on posts delete. Replies to comments delete. Likes delete. Notifications delete. Suddenly you've deleted half your database and the transaction takes 10 minutes.

**Why It Happens:**
CASCADE feels like the right default. Didn't consider data volume. No testing with production-like data. Implicit side effects.

**The Fix:**
```typescript
// WRONG - Cascading delete can spiral
// schema.prisma
model User {
  posts Post[] @relation(onDelete: Cascade)
}

model Post {
  comments Comment[] @relation(onDelete: Cascade)
}
// Deleting user â†’ all posts â†’ all comments â†’ could be millions of rows

// RIGHT - Soft delete
model User {
  deletedAt DateTime?
  posts     Post[]
}

async function deleteUser(userId: string) {
  await db.user.update({
    where: { id: userId },
    data: { deletedAt: new Date() }
  })

  // Background job handles cleanup in batches
  await queue.add('cleanup-user', { userId })
}

// RIGHT - Explicit batched deletion
async function hardDeleteUser(userId: string) {
  // Delete in batches to avoid long transactions
  while (true) {
    const deleted = await db.post.deleteMany({
      where: { authorId: userId },
      take: 1000
    })
    if (deleted.count === 0) break
    await sleep(100) // Don't hammer DB
  }

  await db.user.delete({ where: { id: userId } })
}

// RIGHT - Foreign keys without cascade
// Use RESTRICT or NO ACTION, handle in application
model Post {
  author   User   @relation(fields: [authorId], references: [id], onDelete: Restrict)
  authorId String
}
```

**Detection Pattern:**
- Long-running DELETE statements
- Transaction timeouts on deletes
- Database locks during deletes
- CASCADE DELETE in schema without volume analysis

---

## 9. The Unhandled Async Error

**Severity:** High

**The Trap:**
Background job throws an error. Nobody catches it. Process crashes, or worse, continues in corrupted state. Errors silently swallowed. Data ends up inconsistent and you find out days later.

**Why It Happens:**
Fire-and-forget patterns. Promise rejections not caught. Callbacks without error handling. No global error handlers.

**The Fix:**
```typescript
// WRONG - Unhandled promise rejection
app.post('/api/signup', (req, res) => {
  createUser(req.body) // Not awaited, errors lost
  sendWelcomeEmail(req.body.email) // Not awaited, errors lost
  res.json({ success: true }) // Returns before work done
})

// WRONG - setTimeout/setInterval without error handling
setTimeout(() => {
  processQueue() // If this throws, nobody knows
}, 1000)

// RIGHT - Await and handle errors
app.post('/api/signup', async (req, res, next) => {
  try {
    const user = await createUser(req.body)

    // Background task - use job queue, not fire-and-forget
    await queue.add('send-welcome-email', { userId: user.id })

    res.json({ success: true })
  } catch (error) {
    next(error) // Pass to error handler
  }
})

// RIGHT - Global unhandled rejection handler
process.on('unhandledRejection', (reason, promise) => {
  logger.error('Unhandled Rejection:', { reason, promise })
  // Don't exit - but log and alert
})

// RIGHT - Wrap async intervals
async function safeInterval(fn: () => Promise<void>, ms: number) {
  const wrapped = async () => {
    try {
      await fn()
    } catch (error) {
      logger.error('Interval error:', error)
      // Alert, don't crash
    }
  }

  setInterval(wrapped, ms)
}

// RIGHT - Express async wrapper
const asyncHandler = (fn: AsyncHandler) => (req, res, next) =>
  Promise.resolve(fn(req, res, next)).catch(next)

app.post('/api/users', asyncHandler(async (req, res) => {
  const user = await createUser(req.body)
  res.json(user)
}))
```

**Detection Pattern:**
- UnhandledPromiseRejectionWarning in logs
- Missing data that should exist
- Jobs that "ran" but had no effect
- Silent failures in background processes

---

## 10. The Secrets in Code

**Severity:** Critical

**The Trap:**
API key committed to repo. Database password in config file. JWT secret hardcoded for "convenience." Someone clones the repo, has all your production credentials. Or worse, repo becomes public.

**Why It Happens:**
"Just for development" that goes to production. Copy-paste from docs. No secrets management setup. Not in .gitignore.

**The Fix:**
```typescript
// WRONG - Hardcoded secrets
const STRIPE_KEY = 'sk_live_abc123...'
const JWT_SECRET = 'super-secret-key'
const DB_URL = 'postgres://admin:password123@prod-db.com/main'

// WRONG - Checked into repo
// config.js
module.exports = {
  stripe: 'sk_live_...',
  database: 'postgres://...'
}

// RIGHT - Environment variables
const STRIPE_KEY = process.env.STRIPE_SECRET_KEY
const JWT_SECRET = process.env.JWT_SECRET
const DB_URL = process.env.DATABASE_URL

if (!STRIPE_KEY) {
  throw new Error('STRIPE_SECRET_KEY is required')
}

// RIGHT - .env files (local only, gitignored)
// .env.example (committed, no real values)
STRIPE_SECRET_KEY=sk_test_...
DATABASE_URL=postgres://localhost/dev

// .env (gitignored, real values)
STRIPE_SECRET_KEY=sk_live_...
DATABASE_URL=postgres://user:pass@host/db

// .gitignore
.env
.env.local
.env.*.local

// RIGHT - Secret manager in production
import { SecretManagerServiceClient } from '@google-cloud/secret-manager'

async function getSecret(name: string) {
  const client = new SecretManagerServiceClient()
  const [version] = await client.accessSecretVersion({ name })
  return version.payload.data.toString()
}

// Pre-commit hook to catch secrets
// Use tools like gitleaks, trufflehog
```

**Detection Pattern:**
- Strings that look like keys/passwords in code
- No .env.example in repo
- Secrets in docker-compose.yml
- git log shows sensitive values

---

## 11. The Missing Rate Limit

**Severity:** High

**The Trap:**
Login endpoint has no rate limit. Attacker brute-forces passwords at 1000 requests/second. Either they get in, or your auth service falls over. Either way, you lose.

**Why It Happens:**
"We'll add security later." Didn't seem urgent. Rate limiting is "ops work." Testing doesn't simulate attacks.

**The Fix:**
```typescript
// WRONG - No rate limiting
app.post('/api/login', async (req, res) => {
  const { email, password } = req.body
  const user = await authenticate(email, password)
  res.json({ token: createToken(user) })
})

// RIGHT - Rate limiting with Redis
import rateLimit from 'express-rate-limit'
import RedisStore from 'rate-limit-redis'

// Global rate limit
const globalLimiter = rateLimit({
  store: new RedisStore({ client: redis }),
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 100 // 100 requests per window
})

// Strict limit for auth endpoints
const authLimiter = rateLimit({
  store: new RedisStore({ client: redis }),
  windowMs: 15 * 60 * 1000,
  max: 5, // Only 5 attempts
  keyGenerator: (req) => req.body.email || req.ip, // Per email
  handler: (req, res) => {
    res.status(429).json({
      error: 'Too many login attempts. Try again later.'
    })
  }
})

app.use(globalLimiter)
app.post('/api/login', authLimiter, loginHandler)

// RIGHT - Progressive delays
const failedAttempts = new Map<string, number>()

app.post('/api/login', async (req, res) => {
  const { email } = req.body
  const attempts = failedAttempts.get(email) || 0

  // Progressive delay: 0, 1, 2, 4, 8, 16... seconds
  const delay = Math.min(Math.pow(2, attempts - 1), 60) * 1000
  if (delay > 0) {
    await sleep(delay)
  }

  try {
    const user = await authenticate(req.body)
    failedAttempts.delete(email)
    res.json({ token: createToken(user) })
  } catch {
    failedAttempts.set(email, attempts + 1)
    res.status(401).json({ error: 'Invalid credentials' })
  }
})
```

**Detection Pattern:**
- Auth endpoints without rate limiting
- No failed attempt tracking
- High volume of 401 responses from same IP
- Login success after many failures (brute force succeeded)

---

## 12. The Sync File Processing

**Severity:** High

**The Trap:**
User uploads a file. Server processes it synchronously - parsing, validating, resizing, storing. For a large file, request takes 2 minutes. Connection times out. User retries. Now you're processing the same file twice.

**Why It Happens:**
Synchronous feels simpler. Works fine with small files. No job queue set up. "It's fast enough for now."

**The Fix:**
```typescript
// WRONG - Synchronous processing
app.post('/api/upload', async (req, res) => {
  const file = req.file

  // These can take minutes for large files
  const processed = await processImage(file)
  const uploaded = await uploadToS3(processed)
  const indexed = await indexContent(file)

  res.json({ url: uploaded.url }) // Times out
})

// RIGHT - Async with job queue
import { Queue } from 'bullmq'

const fileQueue = new Queue('file-processing')

app.post('/api/upload', async (req, res) => {
  const file = req.file

  // Quick: Store raw file, create job
  const rawUrl = await uploadRawToS3(file)
  const job = await fileQueue.add('process', {
    fileUrl: rawUrl,
    userId: req.userId
  })

  // Respond immediately
  res.json({
    jobId: job.id,
    status: 'processing',
    checkUrl: `/api/jobs/${job.id}`
  })
})

// Worker processes in background
const worker = new Worker('file-processing', async (job) => {
  const { fileUrl, userId } = job.data

  const file = await downloadFromS3(fileUrl)
  const processed = await processImage(file)
  const finalUrl = await uploadProcessedToS3(processed)

  // Notify user (webhook, websocket, email)
  await notifyUser(userId, { status: 'complete', url: finalUrl })

  return { url: finalUrl }
})

// Status endpoint
app.get('/api/jobs/:id', async (req, res) => {
  const job = await fileQueue.getJob(req.params.id)

  res.json({
    status: await job.getState(),
    progress: job.progress,
    result: job.returnvalue
  })
})
```

**Detection Pattern:**
- Request timeouts on upload endpoints
- Long-running synchronous processes
- No job queue in architecture
- User complaints about "stuck" uploads

## Decision Framework

# Decisions: Backend Engineering

Critical decision points that determine backend architecture success.

---

## Decision 1: Database Selection

**Context:** When choosing your primary database.

**Options:**

| Option | Pros | Cons | Choose When |
|--------|------|------|-------------|
| **PostgreSQL** | Reliable, ACID, JSON support, extensions | Not best for specific workloads | Default for most apps, need transactions |
| **MySQL** | Fast reads, replication, familiarity | Less features than Postgres | High read workload, existing expertise |
| **MongoDB** | Flexible schema, scaling, developer UX | No transactions (mostly), consistency | Document-centric, schema evolving, scaling priority |
| **Redis** | Fast, data structures, pub/sub | Memory-limited, not durable by default | Caching, sessions, real-time features |

**Framework:**
```
Decision tree:

Need ACID transactions across tables?
â”œâ”€â”€ Yes â†’ PostgreSQL or MySQL
â””â”€â”€ No â†’ Continue

Schema changes frequently?
â”œâ”€â”€ Yes â†’ MongoDB
â””â”€â”€ No â†’ Continue

Primarily key-value access patterns?
â”œâ”€â”€ Yes â†’ Redis or DynamoDB
â””â”€â”€ No â†’ Continue

Heavy analytics/timeseries?
â”œâ”€â”€ Yes â†’ ClickHouse, TimescaleDB
â””â”€â”€ No â†’ PostgreSQL (default)

Multi-database is OK:
- Postgres: Primary data (users, orders, products)
- Redis: Cache, sessions, rate limits
- Elasticsearch: Full-text search
- ClickHouse: Analytics

Don't:
- MongoDB for financial data needing transactions
- Postgres for high-velocity time series
- Redis as primary database
```

**Default Recommendation:** PostgreSQL. It handles 90% of use cases, has excellent tooling, and scales further than most realize. Add specialized databases only when PostgreSQL doesn't fit the specific workload.

---

## Decision 2: API Style

**Context:** When designing your API interface.

**Options:**

| Option | Pros | Cons | Choose When |
|--------|------|------|-------------|
| **REST** | Familiar, cacheable, tooling | Over/under fetching, versioning | CRUD-heavy, caching important, public API |
| **GraphQL** | Flexible queries, strong typing, single endpoint | Complexity, caching hard, N+1 | Multiple clients with different needs, complex data graphs |
| **tRPC** | Type-safe, simple, no schema | TypeScript only, tight coupling | Full-stack TypeScript, internal APIs |
| **gRPC** | Performance, streaming, strong contracts | Browser support limited, learning curve | Service-to-service, high performance needed |

**Framework:**
```
Client needs assessment:

Multiple clients with different data needs?
â”œâ”€â”€ Yes â†’ GraphQL or BFF pattern
â””â”€â”€ No â†’ REST or tRPC

Full-stack TypeScript monorepo?
â”œâ”€â”€ Yes â†’ tRPC (simplest)
â””â”€â”€ No â†’ Continue

High-performance service-to-service?
â”œâ”€â”€ Yes â†’ gRPC
â””â”€â”€ No â†’ REST

Public API?
â”œâ”€â”€ Yes â†’ REST (most accessible)
â””â”€â”€ No â†’ Any works

Combination is valid:
- REST for public API
- tRPC for internal
- GraphQL for mobile app
- gRPC between microservices
```

**Default Recommendation:** REST for most cases. It's understood by everyone, works everywhere, and has excellent tooling. Use tRPC for TypeScript monorepos. GraphQL only when you have genuine multi-client query complexity.

---

## Decision 3: Authentication Strategy

**Context:** When implementing user authentication.

**Options:**

| Option | Pros | Cons | Choose When |
|--------|------|------|-------------|
| **Session-based** | Simple, revocable, server control | Stateful, scaling complexity | Traditional web apps, need revocation |
| **JWT** | Stateless, scalable, claims in token | Can't revoke, size grows | Microservices, stateless requirement |
| **OAuth 2.0 / OIDC** | Standard, SSO, delegation | Complexity | Third-party integration, enterprise |
| **Passkeys/WebAuthn** | Phishing-resistant, no passwords | Browser support, UX learning | Security priority, modern apps |

**Framework:**
```
Authentication decision tree:

Need to integrate with identity providers?
â”œâ”€â”€ Yes â†’ OAuth 2.0 / OIDC
â””â”€â”€ No â†’ Continue

Microservices or multiple backend services?
â”œâ”€â”€ Yes â†’ JWT (with short expiry)
â””â”€â”€ No â†’ Continue

Need instant token revocation?
â”œâ”€â”€ Yes â†’ Sessions (or JWT + blocklist)
â””â”€â”€ No â†’ JWT fine

Enterprise customers with SSO requirements?
â”œâ”€â”€ Yes â†’ OIDC (support SAML too)
â””â”€â”€ No â†’ Whatever fits

Practical pattern:
- Short-lived JWT (15 min) + refresh token in httpOnly cookie
- Refresh tokens stored in DB (revocable)
- OIDC layer for third-party auth
- MFA for sensitive operations
```

**Default Recommendation:** Use an auth library or service (NextAuth, Supabase Auth, Auth0). Don't build auth from scratch. If building, use short-lived JWT + refresh tokens with session stored server-side.

---

## Decision 4: Caching Strategy

**Context:** When determining how to cache data.

**Options:**

| Option | Pros | Cons | Choose When |
|--------|------|------|-------------|
| **No cache** | Simple, always fresh | Slower, more DB load | Low traffic, data changes constantly |
| **Application cache (in-memory)** | Fast, no external deps | Per-instance, lost on restart | Single instance, small dataset |
| **Redis/Memcached** | Shared, fast, data structures | Additional infra, network latency | Multiple instances, complex caching |
| **CDN** | Edge caching, global | Limited control, purge delays | Static content, geographic distribution |

**Framework:**
```
Caching decision matrix:

Read/write ratio:
- 100:1 or higher â†’ Cache aggressively
- 10:1 â†’ Cache hot data
- 1:1 or lower â†’ Cache minimally

Staleness tolerance:
- Seconds â†’ In-memory with short TTL
- Minutes â†’ Redis with invalidation
- Hours â†’ CDN appropriate
- Days â†’ CDN with versioning

Cache invalidation strategy:
1. TTL-based: Simple, eventual consistency
2. Event-based: Invalidate on write
3. Version-based: Cache key includes version
4. Write-through: Update cache on write

What to cache:
- Database query results
- Computed values
- API responses
- Session data
- Rate limit counters

What NOT to cache:
- User-specific sensitive data (usually)
- Constantly changing data
- Rarely accessed data
- Large objects (unless justified)
```

**Default Recommendation:** Start without cache. Add Redis when you have a specific performance problem. Use TTL-based invalidation. Only do complex invalidation when TTL doesn't work.

---

## Decision 5: ORM vs Query Builder vs Raw SQL

**Context:** When choosing how to interact with your database.

**Options:**

| Option | Pros | Cons | Choose When |
|--------|------|------|-------------|
| **ORM (Prisma, SQLAlchemy)** | Type safety, migrations, productivity | Magic, performance gotchas, learning curve | Most applications, type safety priority |
| **Query builder (Drizzle, Knex)** | SQL control, type safety, lightweight | Less abstraction, more verbose | SQL knowledge, performance focus |
| **Raw SQL** | Full control, performance, no abstraction | No type safety, SQL injection risk | DBA on team, performance critical |

**Framework:**
```
Team assessment:

Team SQL proficiency:
â”œâ”€â”€ Low â†’ ORM (Prisma)
â”œâ”€â”€ Medium â†’ Query builder (Drizzle)
â””â”€â”€ High â†’ Query builder or raw

Application type:
â”œâ”€â”€ CRUD app â†’ ORM
â”œâ”€â”€ Analytics/reporting â†’ Query builder or raw
â”œâ”€â”€ Complex joins/CTEs â†’ Query builder
â””â”€â”€ Simple queries at scale â†’ ORM fine

Type safety requirements:
â”œâ”€â”€ High â†’ Prisma or Drizzle (both excellent)
â””â”€â”€ Low â†’ Any, including raw

Modern recommendations:
- Prisma: Best DX, great types, some performance overhead
- Drizzle: SQL-like syntax, great types, lightweight
- Knex: Mature, flexible, JS-focused

Complex queries:
// ORM for simple, raw for complex
const users = await prisma.user.findMany({ where: { active: true } })

// Complex analytics? Use raw
const stats = await prisma.$queryRaw`
  SELECT DATE_TRUNC('day', created_at) as day,
         COUNT(*) as signups
  FROM users
  WHERE created_at > ${startDate}
  GROUP BY 1
  ORDER BY 1
`
```

**Default Recommendation:** Prisma for most new projects. The DX and type safety are worth any minor performance overhead. Use raw SQL for complex reporting queries.

---

## Decision 6: Monolith vs Microservices

**Context:** When determining service architecture.

**Options:**

| Option | Pros | Cons | Choose When |
|--------|------|------|-------------|
| **Monolith** | Simple, easy to develop, no network overhead | Harder to scale parts independently | Teams < 30, most startups |
| **Modular monolith** | Code isolation, single deploy, clear boundaries | Discipline required | Growing team, preparing for future |
| **Microservices** | Independent scaling, team ownership, tech flexibility | Operational complexity, network issues | Large teams, clear domain boundaries |

**Framework:**
```
Team size heuristic:
- 1-10 engineers â†’ Monolith
- 10-30 engineers â†’ Modular monolith
- 30+ engineers â†’ Consider microservices

Questions to answer YES before microservices:
â–¡ Do you have clear domain boundaries?
â–¡ Do you have separate teams that could own services?
â–¡ Do you have operational maturity (monitoring, CI/CD, logging)?
â–¡ Do different parts need to scale differently?
â–¡ Do parts need different technology choices?

If not all yes â†’ Stay monolith

Modular monolith pattern:
src/
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ users/     # Could become microservice
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â””â”€â”€ infra/
â”‚   â”œâ”€â”€ orders/    # Could become microservice
â”‚   â””â”€â”€ payments/  # Could become microservice
â””â”€â”€ shared/        # Stays shared

Extraction criteria:
- Module has different team
- Module has different scaling needs
- Module has different deployment frequency
- Module is genuinely independent
```

**Default Recommendation:** Monolith until proven otherwise. The operational overhead of microservices is not justified until you have the team size and clear domain boundaries. Modular monolith gives you the best of both.

---

## Decision 7: Background Job Processing

**Context:** When implementing async task processing.

**Options:**

| Option | Pros | Cons | Choose When |
|--------|------|------|-------------|
| **In-process (setTimeout)** | Simple, no deps | Lost on restart, no retry | Non-critical, dev only |
| **Database-backed (Agenda, pg-boss)** | Use existing DB, transactional | Limited throughput, polling | Simple needs, one database |
| **Redis-backed (BullMQ)** | Fast, features, UI | Additional infra | Most production needs |
| **Cloud queues (SQS, Cloud Tasks)** | Managed, scalable | Vendor lock-in, latency | Cloud-native, scaling needs |

**Framework:**
```
Job requirements assessment:

Volume per day:
- < 1,000 â†’ Database-backed fine
- 1,000-100,000 â†’ Redis-backed (BullMQ)
- > 100,000 â†’ Cloud queues or dedicated

Need guaranteed delivery?
â”œâ”€â”€ Yes â†’ Any except in-process
â””â”€â”€ No â†’ Simple works

Need transactional outbox pattern?
â”œâ”€â”€ Yes â†’ Database-backed or manual
â””â”€â”€ No â†’ Any works

BullMQ is the sweet spot:
- Redis-backed (fast)
- Retry with backoff
- Rate limiting
- Priority queues
- Job scheduling (cron)
- UI dashboard (Bull Board)
- TypeScript support

// Example
const queue = new Queue('emails')

// Add job
await queue.add('send-welcome', { userId }, {
  attempts: 3,
  backoff: { type: 'exponential', delay: 1000 },
  priority: 1
})

// Process
const worker = new Worker('emails', async (job) => {
  await sendEmail(job.data.userId)
})
```

**Default Recommendation:** BullMQ for most production needs. It's the right balance of features and complexity. Use database-backed for very simple needs, cloud queues for very high scale.

---

## Decision 8: Error Handling Strategy

**Context:** When designing your error handling approach.

**Options:**

| Option | Pros | Cons | Choose When |
|--------|------|------|-------------|
| **Exceptions** | Familiar, stack traces, propagation | Hidden control flow, performance | Default for most languages |
| **Result types (Either)** | Explicit, type-safe, composable | Verbose, learning curve | Functional style, critical paths |
| **Error codes** | Simple, portable | No stack trace, magic numbers | Cross-language, simple cases |

**Framework:**
```
Layered error handling:

1. Domain errors (explicit):
class InsufficientBalanceError extends Error {
  constructor(public required: number, public available: number) {
    super(`Insufficient balance: need ${required}, have ${available}`)
  }
}

class OrderNotFoundError extends Error {
  constructor(public orderId: string) {
    super(`Order ${orderId} not found`)
  }
}

2. Service layer (catches and wraps):
async function processOrder(orderId: string) {
  try {
    const order = await orders.get(orderId)
    // ...
  } catch (error) {
    if (error instanceof DatabaseError) {
      throw new ServiceUnavailableError('Database unavailable')
    }
    throw error // Propagate known errors
  }
}

3. Controller layer (maps to HTTP):
app.use((error, req, res, next) => {
  if (error instanceof InsufficientBalanceError) {
    return res.status(422).json({
      error: 'insufficient_balance',
      message: error.message,
      details: { required: error.required, available: error.available }
    })
  }

  if (error instanceof OrderNotFoundError) {
    return res.status(404).json({
      error: 'order_not_found',
      message: error.message
    })
  }

  // Unknown error - don't expose details
  logger.error('Unhandled error:', error)
  res.status(500).json({ error: 'internal_error' })
})

API error format:
{
  "error": "insufficient_balance",
  "message": "Human readable message",
  "details": { ... }  // Optional context
}
```

**Default Recommendation:** Use typed exceptions for domain errors. Catch and translate at boundaries. Never expose internal errors to clients. Always log with context.

---

## Decision 9: Logging and Observability

**Context:** When setting up your observability stack.

**Options:**

| Option | Pros | Cons | Choose When |
|--------|------|------|-------------|
| **Console + Cloud Logging** | Simple, free tier | Limited analysis | Early stage, low volume |
| **ELK Stack** | Powerful, open source | Operational overhead | On-prem, existing expertise |
| **Cloud-native (CloudWatch, Stackdriver)** | Integrated, managed | Vendor lock-in, cost | Cloud-native, integration priority |
| **Third-party (Datadog, New Relic)** | Features, UX, support | Cost at scale | Budget available, need features |

**Framework:**
```
Three pillars of observability:

1. Logs (what happened):
- Structured logging (JSON)
- Log levels (error, warn, info, debug)
- Correlation IDs for request tracing
- No PII in logs

2. Metrics (how much):
- Request rate, latency, errors (RED)
- Resource usage (CPU, memory, connections)
- Business metrics (signups, orders)
- Prometheus + Grafana or cloud equivalent

3. Traces (how it flowed):
- Distributed tracing for request flow
- OpenTelemetry for instrumentation
- Jaeger, Zipkin, or cloud equivalent

Minimum setup:
const logger = pino({
  level: process.env.LOG_LEVEL || 'info'
})

// Request logging middleware
app.use((req, res, next) => {
  req.id = crypto.randomUUID()
  req.log = logger.child({ requestId: req.id })

  const start = Date.now()
  res.on('finish', () => {
    req.log.info({
      method: req.method,
      path: req.path,
      status: res.statusCode,
      duration: Date.now() - start
    })
  })
  next()
})

// In handlers
req.log.info({ orderId }, 'Processing order')
```

**Default Recommendation:** Start with structured logging (Pino) + cloud logging. Add metrics (Prometheus) when you need dashboards. Add tracing when you have multiple services. Upgrade to Datadog/New Relic when budget allows and you're past early stage.

---

## Decision 10: Testing Strategy

**Context:** When establishing your backend testing approach.

**Options:**

| Option | Pros | Cons | Choose When |
|--------|------|------|-------------|
| **Unit tests only** | Fast, isolated | Miss integration issues | Pure functions, libraries |
| **Integration tests only** | Test real behavior | Slower, complex setup | API-focused apps |
| **Unit + Integration** | Balanced coverage | More tests to maintain | Most applications |
| **E2E only** | Real user scenarios | Very slow, flaky | Simple apps, limited resources |

**Framework:**
```
Testing pyramid for backend:

       /\
      /E2E\        Few: Critical paths only
     /------\
    /  API   \     Medium: Key endpoints
   /----------\
  /Integration \   Some: Service + DB
 /--------------\
/      Unit      \ Many: Pure functions

What to test where:

Unit tests:
- Business logic functions
- Validation functions
- Utility functions
- Data transformations

Integration tests:
- Repository methods (with real DB)
- Service methods (with real deps)
- API endpoints (with real server)

E2E tests:
- Critical user flows (signup, purchase)
- Smoke tests after deploy

// Test database setup
beforeAll(async () => {
  await migrate(testDb)
})

beforeEach(async () => {
  await testDb.$executeRaw`TRUNCATE users, orders CASCADE`
})

// Integration test example
describe('OrderService', () => {
  it('creates order with inventory check', async () => {
    // Arrange
    await testDb.products.create({ data: { id: 'p1', stock: 10 } })

    // Act
    const order = await orderService.create({
      items: [{ productId: 'p1', quantity: 5 }]
    })

    // Assert
    expect(order.status).toBe('pending')
    const product = await testDb.products.findUnique({ where: { id: 'p1' } })
    expect(product.stock).toBe(5) // Decremented
  })
})
```

**Default Recommendation:** Focus on integration tests for backend. Unit test complex business logic. Few E2E tests for critical paths. Use real database in tests, not mocks.

## Collaboration

### When to Hand Off

| Trigger | Delegate To | Context |
|---------|-------------|--------|
| `ui|component|react|frontend` | frontend | Backend provides API for frontend |
| `deploy|kubernetes|infrastructure` | devops | Backend needs deployment |
| `security audit|penetration|vulnerability` | cybersecurity | Backend needs security review |
| `test|integration|e2e` | qa-engineering | Backend needs testing strategy |

### Receives Work From

- **frontend**: Frontend needs API endpoints
- **product-management**: Feature requires backend work
- **devops**: Infrastructure needs application changes
- **data-analytics**: Analytics needs data pipeline

### Works Well With

- frontend
- devops
- cybersecurity
- qa-engineering
- analytics

---

## Get the Full Version

This skill has **automated validations**, **detection patterns**, and **structured handoff triggers** that work with the Spawner orchestrator.

```bash
npx vibeship-spawner-skills install
```

Full skill path: `~/.spawner/skills/development/backend/`

**Includes:**
- `skill.yaml` - Structured skill definition
- `sharp-edges.yaml` - Machine-parseable gotchas with detection patterns
- `validations.yaml` - Automated code checks
- `collaboration.yaml` - Handoff triggers for skill orchestration

**Deep content:**
- `patterns.md` - Comprehensive pattern library
- `anti-patterns.md` - What to avoid and why
- `sharp-edges.md` - Detailed gotcha documentation
- `decisions.md` - Decision frameworks

---

*Generated by [VibeShip Spawner](https://github.com/vibeforge1111/vibeship-spawner-skills)*
