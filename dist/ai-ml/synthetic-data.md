# Synthetic Data Generation

> Patterns for generating synthetic data for ML training, testing, and privacy.

**Category:** ai-ml | **Version:** 1.0.0

---

## Patterns

### See full skill for patterns
Contains implementation patterns with code examples


## Anti-Patterns

### See full skill for anti-patterns
Contains anti-patterns with examples


## Sharp Edges (Gotchas)

*Real production issues that cause outages and bugs.*

### [CRITICAL] Synthetic data amplifies existing biases

**Situation:** Generating data from biased source

**Why it happens:**
Synthetic data models learn from real data. If real data has biases:
- Gender representation skewed
- Racial stereotypes in text
- Age discrimination patterns
- Geographic over-representation

Synthetic data doesn't fix bias - it often amplifies it because
models may exaggerate learned patterns.


**Solution:**
```
Audit and balance before and after generation:

```typescript
interface BiasAudit {
  field: string;
  distribution: Record<string, number>;
  expectedDistribution?: Record<string, number>;
  deviation: number;
}

async function auditBias<T extends Record<string, unknown>>(
  data: T[],
  sensitiveFields: string[],
  expectedDistributions?: Record<string, Record<string, number>>
): Promise<BiasAudit[]> {
  const audits: BiasAudit[] = [];

  for (const field of sensitiveFields) {
    const distribution: Record<string, number> = {};

    for (const item of data) {
      const value = String(item[field] ?? "unknown");
      distribution[value] = (distribution[value] || 0) + 1;
    }

    // Normalize to percentages
    const total = data.length;
    for (const key in distribution) {
      distribution[key] = distribution[key] / total;
    }

    // Calculate deviation from expected
    const expected = expectedDistributions?.[field];
    let deviation = 0;
    if (expected) {
      for (const key in expected) {
        deviation += Math.abs((distribution[key] || 0) - expected[key]);
      }
    }

    audits.push({
      field,
      distribution,
      expectedDistribution: expected,
      deviation,
    });
  }

  return audits;
}

// Rebalance synthetic data to match expected distribution
async function rebalanceData<T extends Record<string, unknown>>(
  syntheticData: T[],
  field: string,
  targetDistribution: Record<string, number>,
  generateMore: (category: string, count: number) => Promise<T[]>
): Promise<T[]> {
  const current = await auditBias(syntheticData, [field]);
  const currentDist = current[0].distribution;

  const totalTarget = syntheticData.length;
  const rebalanced: T[] = [];

  for (const [category, targetRatio] of Object.entries(targetDistribution)) {
    const targetCount = Math.round(totalTarget * targetRatio);
    const existing = syntheticData.filter((d) => d[field] === category);

    if (existing.length >= targetCount) {
      // Downsample
      rebalanced.push(...existing.slice(0, targetCount));
    } else {
      // Need more - generate additional
      rebalanced.push(...existing);
      const needed = targetCount - existing.length;
      const additional = await generateMore(category, needed);
      rebalanced.push(...additional);
    }
  }

  return rebalanced;
}
```

```

---

### [CRITICAL] LLM memorizes and reproduces real data

**Situation:** Using LLM for synthetic generation

**Why it happens:**
LLMs trained on internet data may reproduce:
- Real email addresses, phone numbers
- Copyrighted content
- Actual company/person names
- Training data verbatim

Your "synthetic" data may actually be real data from training.


**Solution:**
```
Filter outputs and check for real data:

```typescript
import OpenAI from "openai";

const openai = new OpenAI();

interface PrivacyCheckResult {
  safe: boolean;
  issues: Array<{
    type: "pii" | "memorized" | "copyrighted";
    content: string;
    replacement?: string;
  }>;
  sanitized: unknown;
}

async function checkForMemorization<T>(
  syntheticData: T[],
  options?: { sampleRate?: number }
): Promise<PrivacyCheckResult> {
  const issues: PrivacyCheckResult["issues"] = [];
  const sampleRate = options?.sampleRate ?? 0.1;

  // Sample subset for efficiency
  const sample = syntheticData.filter(() => Math.random() < sampleRate);

  for (const item of sample) {
    const text = JSON.stringify(item);

    // Check for PII patterns
    const piiPatterns = [
      { regex: /\b[\w.+-]+@[\w.-]+\.[a-z]{2,}\b/gi, type: "email" },
      { regex: /\b\d{3}[-.]?\d{3}[-.]?\d{4}\b/g, type: "phone" },
      { regex: /\b\d{3}-\d{2}-\d{4}\b/g, type: "ssn" },
    ];

    for (const { regex, type } of piiPatterns) {
      const matches = text.match(regex);
      if (matches) {
        for (const match of matches) {
          // Verify if it looks like real data (not obviously fake)
          if (!isClearlyFake(match)) {
            issues.push({
              type: "pii",
              content: match,
              replacement: generateFake(type),
            });
          }
        }
      }
    }
  }

  // Sanitize if issues found
  let sanitized = syntheticData;
  if (issues.length > 0) {
    sanitized = syntheticData.map((item) => {
      let text = JSON.stringify(item);
      for (const issue of issues) {
        text = text.replace(issue.content, issue.replacement || "[REDACTED]");
      }
      return JSON.parse(text);
    });
  }

  return {
    safe: issues.length === 0,
    issues,
    sanitized,
  };
}

function isClearlyFake(value: string): boolean {
  // Check for obvious fake patterns
  const fakePatterns = [
    /example\.com$/i,
    /test@/i,
    /fake/i,
    /123-456-7890/,
    /000-00-0000/,
  ];
  return fakePatterns.some((p) => p.test(value));
}

function generateFake(type: string): string {
  switch (type) {
    case "email":
      return `user${Math.random().toString(36).slice(2, 8)}@example.com`;
    case "phone":
      return `555-${Math.floor(Math.random() * 900 + 100)}-${Math.floor(Math.random() * 9000 + 1000)}`;
    case "ssn":
      return "000-00-0000";
    default:
      return "[REDACTED]";
  }
}
```

```

---

### [HIGH] Synthetic data drifts from real distribution

**Situation:** Generating large synthetic datasets

**Why it happens:**
As you generate more data, distribution can drift:
- LLM temperature causes variance accumulation
- Mode collapse in GANs (generates same patterns)
- Rare categories get rarer
- Correlations between fields degrade

Training on drifted data produces poor models.


**Solution:**
```
Monitor and correct distribution continuously:

```typescript
interface DistributionMonitor {
  real: Record<string, number>;
  synthetic: Record<string, number>;
  drift: number;
}

class SyntheticDataPipeline<T extends Record<string, unknown>> {
  private realDistribution: Map<string, Record<string, number>> = new Map();
  private generated: T[] = [];
  private maxDrift = 0.1; // 10% drift threshold

  constructor(private realData: T[], private keyFields: string[]) {
    this.computeRealDistribution();
  }

  private computeRealDistribution(): void {
    for (const field of this.keyFields) {
      const dist: Record<string, number> = {};
      for (const item of this.realData) {
        const value = String(item[field]);
        dist[value] = (dist[value] || 0) + 1;
      }
      // Normalize
      const total = this.realData.length;
      for (const key in dist) {
        dist[key] = dist[key] / total;
      }
      this.realDistribution.set(field, dist);
    }
  }

  async generate(
    count: number,
    generator: () => Promise<T>
  ): Promise<{ data: T[]; driftWarnings: DistributionMonitor[] }> {
    const batchSize = 100;
    const driftWarnings: DistributionMonitor[] = [];

    for (let i = 0; i < count; i += batchSize) {
      const batch: T[] = [];
      for (let j = 0; j < Math.min(batchSize, count - i); j++) {
        batch.push(await generator());
      }

      this.generated.push(...batch);

      // Check drift every batch
      const drift = this.checkDrift();
      const driftedFields = drift.filter((d) => d.drift > this.maxDrift);

      if (driftedFields.length > 0) {
        console.warn("Distribution drift detected:", driftedFields);
        driftWarnings.push(...driftedFields);

        // Optionally: stop and resample, or adjust generator
      }
    }

    return { data: this.generated, driftWarnings };
  }

  private checkDrift(): DistributionMonitor[] {
    const monitors: DistributionMonitor[] = [];

    for (const field of this.keyFields) {
      const real = this.realDistribution.get(field)!;
      const synth: Record<string, number> = {};

      for (const item of this.generated) {
        const value = String(item[field]);
        synth[value] = (synth[value] || 0) + 1;
      }

      // Normalize
      const total = this.generated.length;
      for (const key in synth) {
        synth[key] = synth[key] / total;
      }

      // Calculate JS divergence or simpler metric
      let drift = 0;
      for (const key in real) {
        drift += Math.abs((real[key] || 0) - (synth[key] || 0));
      }

      monitors.push({ real, synthetic: synth, drift });
    }

    return monitors;
  }
}
```

```

---

### [HIGH] Model overfits to synthetic data artifacts

**Situation:** Training only on synthetic data

**Why it happens:**
Synthetic data has artifacts that real data doesn't:
- Consistent formatting from templates
- LLM writing patterns and phrases
- Missing edge cases not in generation
- Unrealistic correlations

Models learn these artifacts instead of real patterns.


**Solution:**
```
Always validate on real data:

```typescript
interface TrainingConfig {
  syntheticData: unknown[];
  realData: unknown[];
  validationSplit: number;
}

function prepareTrainingData(config: TrainingConfig) {
  const { syntheticData, realData, validationSplit } = config;

  // CRITICAL: Validation must be ONLY real data
  const validationSize = Math.floor(realData.length * validationSplit);
  const validationData = realData.slice(0, validationSize);
  const realTrainData = realData.slice(validationSize);

  // Training can mix synthetic + real
  const trainData = [
    ...syntheticData,
    ...realTrainData,
  ];

  // Shuffle
  trainData.sort(() => Math.random() - 0.5);

  return {
    train: trainData,
    validation: validationData, // Only real!
    metadata: {
      syntheticRatio: syntheticData.length / trainData.length,
      realRatio: realTrainData.length / trainData.length,
    },
  };
}

// Monitor for synthetic overfitting
interface OverfitMetrics {
  trainLoss: number;
  syntheticValLoss: number;
  realValLoss: number;
  gap: number; // Large gap = overfitting to synthetic
}

function checkOverfitting(metrics: OverfitMetrics): string[] {
  const warnings: string[] = [];

  if (metrics.realValLoss - metrics.syntheticValLoss > 0.2) {
    warnings.push("Model performs much better on synthetic than real data. Likely overfitting to synthetic artifacts.");
  }

  if (metrics.trainLoss < 0.1 && metrics.realValLoss > 0.5) {
    warnings.push("Training loss very low but real validation high. Check for data leakage or artifact learning.");
  }

  return warnings;
}
```

```

---

### [HIGH] Synthetic data doesn't guarantee privacy

**Situation:** Using synthetic data for privacy

**Why it happens:**
Common misconception: "synthetic = private". Reality:
- Outliers in real data may be reproduced exactly
- Rare combinations are memorizable
- Membership inference attacks still work
- Attribute inference from correlations

Synthetic data needs explicit privacy measures.


**Solution:**
```
Add differential privacy to generation:

```typescript
interface DPConfig {
  epsilon: number; // Lower = more private, less accurate
  delta: number;
  sensitivity: number;
}

// Add Laplacian noise for differential privacy
function addLaplacianNoise(value: number, config: DPConfig): number {
  const scale = config.sensitivity / config.epsilon;
  // Laplacian distribution
  const u = Math.random() - 0.5;
  const noise = -scale * Math.sign(u) * Math.log(1 - 2 * Math.abs(u));
  return value + noise;
}

// For categorical data, use randomized response
function randomizedResponse<T>(
  value: T,
  allValues: T[],
  epsilon: number
): T {
  const p = Math.exp(epsilon) / (Math.exp(epsilon) + allValues.length - 1);

  if (Math.random() < p) {
    return value; // Report true value
  } else {
    // Report random value
    const otherValues = allValues.filter((v) => v !== value);
    return otherValues[Math.floor(Math.random() * otherValues.length)];
  }
}

// Validate privacy with membership inference test
async function testMembershipInference(
  realData: unknown[],
  syntheticData: unknown[],
  testSplit: number = 0.2
): Promise<{
  accuracy: number;
  privacyRisk: "low" | "medium" | "high";
}> {
  // Split real data: some in training, some held out
  const splitIdx = Math.floor(realData.length * (1 - testSplit));
  const trainReal = realData.slice(0, splitIdx);
  const heldOut = realData.slice(splitIdx);

  // Attacker tries to distinguish:
  // 1. Records that were used to generate synthetic (trainReal)
  // 2. Records never seen (heldOut)

  // Simple nearest-neighbor attack
  let correct = 0;
  const total = trainReal.length + heldOut.length;

  for (const record of trainReal) {
    const dist = nearestDistance(record, syntheticData);
    if (dist < 0.1) correct++; // Guesses "in training" if very similar
  }

  for (const record of heldOut) {
    const dist = nearestDistance(record, syntheticData);
    if (dist >= 0.1) correct++; // Guesses "not in training" if dissimilar
  }

  const accuracy = correct / total;

  return {
    accuracy,
    privacyRisk: accuracy > 0.7 ? "high" : accuracy > 0.55 ? "medium" : "low",
  };
}

function nearestDistance(record: unknown, dataset: unknown[]): number {
  // Simplified - would use proper distance metric
  const recordStr = JSON.stringify(record);
  let minDist = Infinity;

  for (const item of dataset) {
    const dist = levenshtein(recordStr, JSON.stringify(item));
    if (dist < minDist) minDist = dist;
  }

  return minDist / recordStr.length; // Normalize
}
```

```

---

### [MEDIUM] Generator produces repetitive outputs

**Situation:** Using GANs or low temperature

**Why it happens:**
Mode collapse happens when generator:
- Only produces subset of possible outputs
- Ignores rare categories
- Converges to "safe" average patterns

Results in low diversity, missing edge cases.


**Solution:**
```
Monitor diversity and adjust generation:

```typescript
interface DiversityMetrics {
  uniqueRatio: number;
  entropyByField: Record<string, number>;
  categoryConverage: Record<string, number>;
  repetitionRate: number;
}

function measureDiversity<T extends Record<string, unknown>>(
  data: T[],
  categoricalFields: string[]
): DiversityMetrics {
  // Unique ratio
  const unique = new Set(data.map((d) => JSON.stringify(d)));
  const uniqueRatio = unique.size / data.length;

  // Entropy per categorical field
  const entropyByField: Record<string, number> = {};
  const categoryConverage: Record<string, number> = {};

  for (const field of categoricalFields) {
    const counts: Record<string, number> = {};
    for (const item of data) {
      const value = String(item[field]);
      counts[value] = (counts[value] || 0) + 1;
    }

    // Entropy
    let entropy = 0;
    for (const count of Object.values(counts)) {
      const p = count / data.length;
      entropy -= p * Math.log2(p);
    }
    entropyByField[field] = entropy;

    // Coverage (categories represented)
    categoryConverage[field] = Object.keys(counts).length;
  }

  // Repetition rate (consecutive duplicates)
  let repetitions = 0;
  for (let i = 1; i < data.length; i++) {
    if (JSON.stringify(data[i]) === JSON.stringify(data[i - 1])) {
      repetitions++;
    }
  }

  return {
    uniqueRatio,
    entropyByField,
    categoryConverage,
    repetitionRate: repetitions / data.length,
  };
}

// Adaptive temperature based on diversity
function adaptiveTemperature(
  currentDiversity: DiversityMetrics,
  targetUniqueRatio: number = 0.95
): number {
  const gap = targetUniqueRatio - currentDiversity.uniqueRatio;

  // If diversity too low, increase temperature
  // If diversity fine, use base temperature
  const baseTemp = 0.8;
  const adjustment = Math.max(0, gap) * 0.5;

  return Math.min(1.0, baseTemp + adjustment);
}
```

```

---

### [MEDIUM] LLM generates factually incorrect data

**Situation:** Generating domain-specific data

**Why it happens:**
LLMs hallucinate plausible-looking but wrong data:
- Invalid zip codes for cities
- Impossible date combinations
- Wrong technical specifications
- Inconsistent within same record

Training on hallucinated data teaches wrong patterns.


**Solution:**
```
Validate domain-specific constraints:

```typescript
import { z } from "zod";

// Define strict domain constraints
const USAddress = z.object({
  street: z.string(),
  city: z.string(),
  state: z.string().regex(/^[A-Z]{2}$/),
  zipCode: z.string().regex(/^\d{5}(-\d{4})?$/),
}).refine(
  async (data) => {
    // Validate zip matches city/state
    return await validateZipCityState(data.zipCode, data.city, data.state);
  },
  { message: "ZIP code doesn't match city/state" }
);

const DateRange = z.object({
  startDate: z.date(),
  endDate: z.date(),
}).refine(
  (data) => data.startDate <= data.endDate,
  { message: "Start date must be before end date" }
);

// Domain-specific validation
const MedicalRecord = z.object({
  age: z.number().min(0).max(150),
  diagnosisCode: z.string(),
  medications: z.array(z.string()),
  bloodPressure: z.object({
    systolic: z.number().min(70).max(250),
    diastolic: z.number().min(40).max(150),
  }),
}).refine(
  (data) => data.bloodPressure.systolic > data.bloodPressure.diastolic,
  { message: "Systolic must be higher than diastolic" }
);

// Validate and filter synthetic data
async function validateSyntheticBatch<T>(
  data: unknown[],
  schema: z.ZodType<T>
): Promise<{
  valid: T[];
  invalid: unknown[];
  errorRate: number;
}> {
  const valid: T[] = [];
  const invalid: unknown[] = [];

  for (const item of data) {
    const result = await schema.safeParseAsync(item);
    if (result.success) {
      valid.push(result.data);
    } else {
      invalid.push({ item, errors: result.error.issues });
    }
  }

  return {
    valid,
    invalid,
    errorRate: invalid.length / data.length,
  };
}
```

```

---

### [MEDIUM] LLM generation costs spiral out of control

**Situation:** Generating large datasets with LLM

**Why it happens:**
Generating 10,000 examples with GPT-4:
- ~500 tokens per example (input + output)
- 5M tokens total
- ~$50-100 for GPT-4o
- ~$150-300 for Claude Sonnet

Iteration and regeneration multiply costs.


**Solution:**
```
Estimate costs and use tiered approach:

```typescript
interface CostEstimate {
  inputTokens: number;
  outputTokens: number;
  estimatedCost: number;
  model: string;
}

const MODEL_PRICING = {
  "gpt-4o": { input: 2.5, output: 10 }, // per 1M tokens
  "gpt-4o-mini": { input: 0.15, output: 0.6 },
  "claude-sonnet-4-20250514": { input: 3, output: 15 },
  "claude-3-5-haiku-20241022": { input: 0.8, output: 4 },
};

function estimateGenerationCost(
  exampleCount: number,
  avgInputTokens: number,
  avgOutputTokens: number,
  model: keyof typeof MODEL_PRICING
): CostEstimate {
  const pricing = MODEL_PRICING[model];
  const totalInput = exampleCount * avgInputTokens;
  const totalOutput = exampleCount * avgOutputTokens;

  const inputCost = (totalInput / 1_000_000) * pricing.input;
  const outputCost = (totalOutput / 1_000_000) * pricing.output;

  return {
    inputTokens: totalInput,
    outputTokens: totalOutput,
    estimatedCost: inputCost + outputCost,
    model,
  };
}

// Tiered generation: cheap model for quantity, expensive for quality
async function tieredGeneration<T>(
  count: number,
  generator: (model: string) => Promise<T>
): Promise<T[]> {
  const results: T[] = [];

  // 80% with cheap model
  const cheapCount = Math.floor(count * 0.8);
  for (let i = 0; i < cheapCount; i++) {
    results.push(await generator("gpt-4o-mini"));
  }

  // 20% with quality model for diversity
  const qualityCount = count - cheapCount;
  for (let i = 0; i < qualityCount; i++) {
    results.push(await generator("gpt-4o"));
  }

  return results;
}
```

```

---

## Collaboration

---

## Get the Full Version

This skill has **automated validations**, **detection patterns**, and **structured handoff triggers** that work with the Spawner orchestrator.

```bash
npx vibeship-spawner-skills install
```

Full skill path: `~/.spawner/skills/ai-ml/synthetic-data/`

**Includes:**
- `skill.yaml` - Structured skill definition
- `sharp-edges.yaml` - Machine-parseable gotchas with detection patterns
- `validations.yaml` - Automated code checks
- `collaboration.yaml` - Handoff triggers for skill orchestration

---

*Generated by [VibeShip Spawner](https://github.com/vibeforge1111/vibeship-spawner-skills)*
