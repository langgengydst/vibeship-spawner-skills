# Data Reproducibility

> Infrastructure and practices for reproducible computational research.
Covers environment management, data versioning, code documentation,
and sharing protocols that enable others to reproduce your results.


**Category:** science | **Version:** 1.0.0

---

## Patterns


## Sharp Edges (Gotchas)

*Real production issues that cause outages and bugs.*

### [CRITICAL] Same code, same seed, different results on different GPUs

**Why it happens:**
GPU operations often use non-deterministic algorithms for speed.
Different hardware has different floating point precision.
Order of operations affects floating point results.


**Solution:**
```
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.use_deterministic_algorithms(True)
# Note: Some ops don't have deterministic implementations

```

**Symptoms:**
- Results differ between local and cloud
- CI produces different numbers than laptop
- Model checkpoints don't reproduce exactly

---

### [CRITICAL] pip freeze misses system libraries your code depends on

**Why it happens:**
Python packages often wrap system libraries (BLAS, LAPACK, OpenSSL).
pip/conda can't capture system-level dependencies.
Different systems have different versions installed.


**Solution:**
```
1. Use Docker for complete isolation
2. Document system requirements in README
3. Use conda for scientific packages (bundles system libs)
4. Test in clean environment before publishing

```

**Symptoms:**
- Works on your machine, fails on fresh install
- Cryptic import errors about missing .so files
- Different numerical results on different systems

---

### [HIGH] Upstream data source changed, breaking reproducibility

**Solution:**
```
1. Version data with DVC or similar
2. Hash all input data in manifests
3. Archive exact dataset used for publications
4. Never depend on mutable data sources for research

```

**Symptoms:**
- Model performance suddenly drops
- Can't reproduce old results with current data
- Results from paper don't match current code

---

### [HIGH] datetime.now() in features breaks reproducibility

**Solution:**
```
# Bad
df['age'] = (datetime.now() - df['birth_date']).days / 365

# Good: Use fixed reference date
REFERENCE_DATE = datetime(2024, 1, 1)
df['age'] = (REFERENCE_DATE - df['birth_date']).days / 365

```

**Symptoms:**
- Different results when run at different times
- Time-based features change between runs

---

### [MEDIUM] Hash randomization affects iteration order

**Solution:**
```
os.environ['PYTHONHASHSEED'] = '0'
# Or use Python 3.7+ where dicts maintain insertion order

```

**Symptoms:**
- Different feature order in older Python
- PYTHONHASHSEED not set

---

## Collaboration

### When to Hand Off

| Trigger | Delegate To | Context |
|---------|-------------|--------|
| `docker|container` | docker-containerization | Need containerized environment |
| `track experiment|mlflow` | ml-ops | Need ML experiment tracking |

### Receives Work From

- **scientific-method**: Need reproducible experimental setup
- **ml-ops**: Need ML experiment reproducibility

---

## Get the Full Version

This skill has **automated validations**, **detection patterns**, and **structured handoff triggers** that work with the Spawner orchestrator.

```bash
npx vibeship-spawner-skills install
```

Full skill path: `~/.spawner/skills/science/data-reproducibility/`

**Includes:**
- `skill.yaml` - Structured skill definition
- `sharp-edges.yaml` - Machine-parseable gotchas with detection patterns
- `validations.yaml` - Automated code checks
- `collaboration.yaml` - Handoff triggers for skill orchestration

---

*Generated by [VibeShip Spawner](https://github.com/vibeforge1111/vibeship-spawner-skills)*
